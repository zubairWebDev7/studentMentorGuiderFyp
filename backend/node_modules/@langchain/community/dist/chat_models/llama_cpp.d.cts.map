{"version":3,"file":"llama_cpp.d.cts","names":["LlamaModel","LlamaContext","LlamaChatSession","ChatHistoryItem","SimpleChatModel","BaseChatModelParams","BaseLanguageModelCallOptions","CallbackManagerForLLMRun","BaseMessage","ChatGenerationChunk","LlamaBaseCppInputs","LlamaCppInputs","LlamaCppCallOptions","ChatLlamaCpp","Promise","AsyncGenerator"],"sources":["../../src/chat_models/llama_cpp.d.ts"],"sourcesContent":["import { LlamaModel, LlamaContext, LlamaChatSession, ChatHistoryItem } from \"node-llama-cpp\";\nimport { SimpleChatModel, type BaseChatModelParams } from \"@langchain/core/language_models/chat_models\";\nimport type { BaseLanguageModelCallOptions } from \"@langchain/core/language_models/base\";\nimport { CallbackManagerForLLMRun } from \"@langchain/core/callbacks/manager\";\nimport { BaseMessage } from \"@langchain/core/messages\";\nimport { ChatGenerationChunk } from \"@langchain/core/outputs\";\nimport { LlamaBaseCppInputs } from \"../utils/llama_cpp.js\";\n/**\n * Note that the modelPath is the only required parameter. For testing you\n * can set this in the environment variable `LLAMA_PATH`.\n */\nexport interface LlamaCppInputs extends LlamaBaseCppInputs, BaseChatModelParams {\n}\nexport interface LlamaCppCallOptions extends BaseLanguageModelCallOptions {\n    /** The maximum number of tokens the response should contain. */\n    maxTokens?: number;\n    /** A function called when matching the provided token array */\n    onToken?: (tokens: number[]) => void;\n}\n/**\n *  To use this model you need to have the `node-llama-cpp` module installed.\n *  This can be installed using `npm install -S node-llama-cpp` and the minimum\n *  version supported in version 2.0.0.\n *  This also requires that have a locally built version of Llama3 installed.\n * @example\n * ```typescript\n * // Initialize the ChatLlamaCpp model with the path to the model binary file.\n * const model = await ChatLlamaCpp.initialize({\n *   modelPath: \"/Replace/with/path/to/your/model/gguf-llama3-Q4_0.bin\",\n *   temperature: 0.5,\n * });\n *\n * // Call the model with a message and await the response.\n * const response = await model.invoke([\n *   new HumanMessage({ content: \"My name is John.\" }),\n * ]);\n *\n * // Log the response to the console.\n * console.log({ response });\n *\n * ```\n */\nexport declare class ChatLlamaCpp extends SimpleChatModel<LlamaCppCallOptions> {\n    static inputs: LlamaCppInputs;\n    maxTokens?: number;\n    temperature?: number;\n    topK?: number;\n    topP?: number;\n    trimWhitespaceSuffix?: boolean;\n    _model: LlamaModel;\n    _context: LlamaContext;\n    _session: LlamaChatSession | null;\n    lc_serializable: boolean;\n    static lc_name(): string;\n    constructor(inputs: LlamaCppInputs);\n    /**\n     * Initializes the llama_cpp model for usage in the chat models wrapper.\n     * @param inputs - the inputs passed onto the model.\n     * @returns A Promise that resolves to the ChatLlamaCpp type class.\n     */\n    static initialize(inputs: LlamaBaseCppInputs): Promise<ChatLlamaCpp>;\n    _llmType(): string;\n    /** @ignore */\n    _combineLLMOutput(): {};\n    invocationParams(): {\n        maxTokens: number | undefined;\n        temperature: number | undefined;\n        topK: number | undefined;\n        topP: number | undefined;\n        trimWhitespaceSuffix: boolean | undefined;\n    };\n    /** @ignore */\n    _call(messages: BaseMessage[], options: this[\"ParsedCallOptions\"], runManager?: CallbackManagerForLLMRun): Promise<string>;\n    _streamResponseChunks(input: BaseMessage[], _options: this[\"ParsedCallOptions\"], runManager?: CallbackManagerForLLMRun): AsyncGenerator<ChatGenerationChunk>;\n    protected _buildSession(messages: BaseMessage[]): string;\n    protected _convertMessagesToInteractions(messages: BaseMessage[]): ChatHistoryItem[];\n    protected _buildPrompt(input: BaseMessage[]): string;\n}\n//# sourceMappingURL=llama_cpp.d.ts.map"],"mappings":";;;;;;;;;;;;AAWA;AAEA;AA6BqBa,UA/BJF,cAAAA,SAAuBD,kBA+BP,EA/B2BL,mBA+B3B,CAAA;AACdM,UA9BFC,mBAAAA,SAA4BN,4BA8B1BK,CAAAA;EAMPX;EACEC,SAAAA,CAAAA,EAAAA,MAAAA;EACAC;EAGUS,OAAAA,CAAAA,EAAAA,CAAAA,MAAAA,EAAAA,MAAAA,EAAAA,EAAAA,GAAAA,IAAAA;;;;;;;;;;;;;;;;AAZiC;;;;;;;;;cAApCE,YAAAA,SAAqBT,gBAAgBQ;iBACvCD;;;;;;UAMPX;YACEC;YACAC;;;sBAGUS;;;;;;4BAMMD,qBAAqBI,QAAQD;;;;;;;;;;;;kBAYvCL,gEAAgED,2BAA2BO;+BAC9EN,iEAAiED,2BAA2BQ,eAAeN;oCACtGD;qDACiBA,gBAAgBL;gCACrCK"}