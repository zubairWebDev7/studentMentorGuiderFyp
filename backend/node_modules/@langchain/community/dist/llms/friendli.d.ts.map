{"version":3,"file":"friendli.d.ts","names":["CallbackManagerForLLMRun","BaseLLMCallOptions","BaseLLMParams","LLM","GenerationChunk","FriendliParams","Record","Friendli","Promise","AsyncGenerator"],"sources":["../../src/llms/friendli.d.ts"],"sourcesContent":["import { CallbackManagerForLLMRun } from \"@langchain/core/callbacks/manager\";\nimport { type BaseLLMCallOptions, type BaseLLMParams, LLM } from \"@langchain/core/language_models/llms\";\nimport { GenerationChunk } from \"@langchain/core/outputs\";\n/**\n * The FriendliParams interface defines the input parameters for\n * the Friendli class.\n */\nexport interface FriendliParams extends BaseLLMParams {\n    /**\n     * Model name to use.\n     */\n    model?: string;\n    /**\n     * Base endpoint url.\n     */\n    baseUrl?: string;\n    /**\n     * Friendli personal access token to run as.\n     */\n    friendliToken?: string;\n    /**\n     * Friendli team ID to run as.\n     */\n    friendliTeam?: string;\n    /**\n     * Number between -2.0 and 2.0. Positive values penalizes tokens that have been\n     * sampled, taking into account their frequency in the preceding text. This\n     * penalization diminishes the model's tendency to reproduce identical lines\n     * verbatim.\n     */\n    frequencyPenalty?: number;\n    /**\n     * Number between -2.0 and 2.0. Positive values penalizes tokens that have been\n     * sampled at least once in the existing text.\n     * presence_penalty: Optional[float] = None\n     * The maximum number of tokens to generate. The length of your input tokens plus\n     * `max_tokens` should not exceed the model's maximum length (e.g., 2048 for OpenAI\n     * GPT-3)\n     */\n    maxTokens?: number;\n    /**\n     * When one of the stop phrases appears in the generation result, the API will stop\n     * generation. The phrase is included in the generated result. If you are using\n     * beam search, all of the active beams should contain the stop phrase to terminate\n     * generation. Before checking whether a stop phrase is included in the result, the\n     * phrase is converted into tokens.\n     */\n    stop?: string[];\n    /**\n     * Sampling temperature. Smaller temperature makes the generation result closer to\n     * greedy, argmax (i.e., `top_k = 1`) sampling. If it is `None`, then 1.0 is used.\n     */\n    temperature?: number;\n    /**\n     * Tokens comprising the top `top_p` probability mass are kept for sampling. Numbers\n     * between 0.0 (exclusive) and 1.0 (inclusive) are allowed. If it is `None`, then 1.0\n     * is used by default.\n     */\n    topP?: number;\n    /**\n     * Additional kwargs to pass to the model.\n     */\n    modelKwargs?: Record<string, unknown>;\n}\n/**\n * The Friendli class is used to interact with Friendli inference Endpoint models.\n * This requires your Friendli Token and Friendli Team which is autoloaded if not specified.\n */\nexport declare class Friendli extends LLM<BaseLLMCallOptions> {\n    lc_serializable: boolean;\n    static lc_name(): string;\n    get lc_secrets(): {\n        [key: string]: string;\n    } | undefined;\n    model: string;\n    baseUrl: string;\n    friendliToken?: string;\n    friendliTeam?: string;\n    frequencyPenalty?: number;\n    maxTokens?: number;\n    stop?: string[];\n    temperature?: number;\n    topP?: number;\n    modelKwargs?: Record<string, unknown>;\n    constructor(fields: FriendliParams);\n    _llmType(): string;\n    private constructHeaders;\n    private constructBody;\n    /**\n     * Calls the Friendli endpoint and retrieves the result.\n     * @param {string} prompt The input prompt.\n     * @returns {Promise<string>} A promise that resolves to the generated string.\n     */\n    /** @ignore */\n    _call(prompt: string, _options: this[\"ParsedCallOptions\"]): Promise<string>;\n    _streamResponseChunks(prompt: string, _options: this[\"ParsedCallOptions\"], runManager?: CallbackManagerForLLMRun): AsyncGenerator<GenerationChunk>;\n}\n//# sourceMappingURL=friendli.d.ts.map"],"mappings":";;;;;;;;AAOA;AA6DA;AAA0CC,UA7DzBI,cAAAA,SAAuBH,aA6DED,CAAAA;EAexBK;;;EAY0EN,KAAAA,CAAAA,EAAAA,MAAAA;EAA0CI;;;EA3B7F,OAAA,CAAA,EAAA,MAAA;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;gBANvBE;;;;;;cAMGC,QAAAA,SAAiBJ,IAAIF;;;;;;;;;;;;;;;gBAexBK;sBACMD;;;;;;;;;;8DAUwCG;0FAC4BR,2BAA2BS,eAAeL"}