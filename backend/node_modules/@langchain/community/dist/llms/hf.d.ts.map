{"version":3,"file":"hf.d.ts","names":["CallbackManagerForLLMRun","LLM","BaseLLMParams","GenerationChunk","HFInput","HuggingFaceInference","Partial","AsyncGenerator","Promise","_huggingface_inference0","HfInference"],"sources":["../../src/llms/hf.d.ts"],"sourcesContent":["import { CallbackManagerForLLMRun } from \"@langchain/core/callbacks/manager\";\nimport { LLM, type BaseLLMParams } from \"@langchain/core/language_models/llms\";\nimport { GenerationChunk } from \"@langchain/core/outputs\";\n/**\n * Interface defining the parameters for configuring the Hugging Face\n * model for text generation.\n */\nexport interface HFInput {\n    /** Model to use */\n    model: string;\n    /** Custom inference endpoint URL to use */\n    endpointUrl?: string;\n    /** Sampling temperature to use */\n    temperature?: number;\n    /**\n     * Maximum number of tokens to generate in the completion.\n     */\n    maxTokens?: number;\n    /**\n     * The model will stop generating text when one of the strings in the list is generated.\n     */\n    stopSequences?: string[];\n    /** Total probability mass of tokens to consider at each step */\n    topP?: number;\n    /** Integer to define the top tokens considered within the sample operation to create new text. */\n    topK?: number;\n    /** Penalizes repeated tokens according to frequency */\n    frequencyPenalty?: number;\n    /** API key to use. */\n    apiKey?: string;\n    /**\n     * Credentials to use for the request. If this is a string, it will be passed straight on. If it's a boolean, true will be \"include\" and false will not send credentials at all.\n     */\n    includeCredentials?: string | boolean;\n}\n/**\n * Class implementing the Large Language Model (LLM) interface using the\n * Hugging Face Inference API for text generation.\n * @example\n * ```typescript\n * const model = new HuggingFaceInference({\n *   model: \"gpt2\",\n *   temperature: 0.7,\n *   maxTokens: 50,\n * });\n *\n * const res = await model.invoke(\n *   \"Question: What would be a good company name for a company that makes colorful socks?\\nAnswer:\"\n * );\n * console.log({ res });\n * ```\n */\nexport declare class HuggingFaceInference extends LLM implements HFInput {\n    lc_serializable: boolean;\n    get lc_secrets(): {\n        [key: string]: string;\n    } | undefined;\n    model: string;\n    temperature: number | undefined;\n    maxTokens: number | undefined;\n    stopSequences: string[] | undefined;\n    topP: number | undefined;\n    topK: number | undefined;\n    frequencyPenalty: number | undefined;\n    apiKey: string | undefined;\n    endpointUrl: string | undefined;\n    includeCredentials: string | boolean | undefined;\n    constructor(fields?: Partial<HFInput> & BaseLLMParams);\n    _llmType(): string;\n    invocationParams(options?: this[\"ParsedCallOptions\"]): {\n        model: string;\n        parameters: {\n            return_full_text: boolean;\n            temperature: number | undefined;\n            max_new_tokens: number | undefined;\n            stop: string[] | undefined;\n            top_p: number | undefined;\n            top_k: number | undefined;\n            repetition_penalty: number | undefined;\n        };\n    };\n    _streamResponseChunks(prompt: string, options: this[\"ParsedCallOptions\"], runManager?: CallbackManagerForLLMRun): AsyncGenerator<GenerationChunk>;\n    /** @ignore */\n    _call(prompt: string, options: this[\"ParsedCallOptions\"]): Promise<string>;\n    private _prepareHFInference;\n    /** @ignore */\n    static imports(): Promise<{\n        HfInference: typeof import(\"@huggingface/inference\").HfInference;\n    }>;\n}\n//# sourceMappingURL=hf.d.ts.map"],"mappings":";;;;;;;;;;AAOA;AA6CqBK,UA7CJD,OAAAA,CA6CIC;EAeYD;EAARE,KAAAA,EAAAA,MAAAA;EAAmBJ;EAc+CF,WAAAA,CAAAA,EAAAA,MAAAA;EAA0CG;EAAfI,WAAAA,CAAAA,EAAAA,MAAAA;EAEvDC;;;EA/BbP,SAAAA,CAAAA,EAAAA,MAAAA;EAAeG;AAAO;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;cAAnDC,oBAAAA,SAA6BJ,GAAAA,YAAeG;;;;;;;;;;;;;;;uBAexCE,QAAQF,WAAWF;;;;;;;;;;;;;;yFAc+CF,2BAA2BO,eAAeJ;;6DAEtEK;;;oBAGzCA;wBAHgDC,uBAAAA,CAITC"}