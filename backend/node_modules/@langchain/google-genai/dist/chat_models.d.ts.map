{"version":3,"file":"chat_models.d.ts","names":["GenerateContentRequest","SafetySetting","Part","GenerativeAIPart","ModelParams","RequestOptions","CachedContent","Schema","CallbackManagerForLLMRun","AIMessageChunk","BaseMessage","ChatGenerationChunk","ChatResult","BaseChatModel","BaseChatModelCallOptions","LangSmithParams","BaseChatModelParams","ModelProfile","BaseLanguageModelInput","StructuredOutputMethodOptions","Runnable","InteropZodType","GoogleGenerativeAIThinkingConfig","GoogleGenerativeAIToolType","BaseMessageExamplePair","GoogleGenerativeAIChatCallOptions","GoogleGenerativeAIChatInput","Record","Pick","ChatGoogleGenerativeAI","RunOutput","Partial","Omit","Promise","AsyncGenerator","_google_generative_ai0","GenerateContentResult"],"sources":["../src/chat_models.d.ts"],"sourcesContent":["import { GenerateContentRequest, SafetySetting, Part as GenerativeAIPart, ModelParams, RequestOptions, type CachedContent, Schema } from \"@google/generative-ai\";\nimport { CallbackManagerForLLMRun } from \"@langchain/core/callbacks/manager\";\nimport { AIMessageChunk, BaseMessage } from \"@langchain/core/messages\";\nimport { ChatGenerationChunk, ChatResult } from \"@langchain/core/outputs\";\nimport { BaseChatModel, type BaseChatModelCallOptions, type LangSmithParams, type BaseChatModelParams } from \"@langchain/core/language_models/chat_models\";\nimport { ModelProfile } from \"@langchain/core/language_models/profile\";\nimport { BaseLanguageModelInput, StructuredOutputMethodOptions } from \"@langchain/core/language_models/base\";\nimport { Runnable } from \"@langchain/core/runnables\";\nimport { InteropZodType } from \"@langchain/core/utils/types\";\nimport { GoogleGenerativeAIThinkingConfig, GoogleGenerativeAIToolType } from \"./types.js\";\nexport type BaseMessageExamplePair = {\n    input: BaseMessage;\n    output: BaseMessage;\n};\nexport interface GoogleGenerativeAIChatCallOptions extends BaseChatModelCallOptions {\n    tools?: GoogleGenerativeAIToolType[];\n    /**\n     * Allowed functions to call when the mode is \"any\".\n     * If empty, any one of the provided functions are called.\n     */\n    allowedFunctionNames?: string[];\n    /**\n     * Whether or not to include usage data, like token counts\n     * in the streamed response chunks.\n     * @default true\n     */\n    streamUsage?: boolean;\n    /**\n     * JSON schema to be returned by the model.\n     */\n    responseSchema?: Schema;\n}\n/**\n * An interface defining the input to the ChatGoogleGenerativeAI class.\n */\nexport interface GoogleGenerativeAIChatInput extends BaseChatModelParams, Pick<GoogleGenerativeAIChatCallOptions, \"streamUsage\"> {\n    /**\n     * Model Name to use\n     *\n     * Note: The format must follow the pattern - `{model}`\n     */\n    model: string;\n    /**\n     * Controls the randomness of the output.\n     *\n     * Values can range from [0.0,2.0], inclusive. A value closer to 2.0\n     * will produce responses that are more varied and creative, while\n     * a value closer to 0.0 will typically result in less surprising\n     * responses from the model.\n     *\n     * Note: The default value varies by model\n     */\n    temperature?: number;\n    /**\n     * Maximum number of tokens to generate in the completion.\n     */\n    maxOutputTokens?: number;\n    /**\n     * Top-p changes how the model selects tokens for output.\n     *\n     * Tokens are selected from most probable to least until the sum\n     * of their probabilities equals the top-p value.\n     *\n     * For example, if tokens A, B, and C have a probability of\n     * .3, .2, and .1 and the top-p value is .5, then the model will\n     * select either A or B as the next token (using temperature).\n     *\n     * Note: The default value varies by model\n     */\n    topP?: number;\n    /**\n     * Top-k changes how the model selects tokens for output.\n     *\n     * A top-k of 1 means the selected token is the most probable among\n     * all tokens in the model's vocabulary (also called greedy decoding),\n     * while a top-k of 3 means that the next token is selected from\n     * among the 3 most probable tokens (using temperature).\n     *\n     * Note: The default value varies by model\n     */\n    topK?: number;\n    /**\n     * The set of character sequences (up to 5) that will stop output generation.\n     * If specified, the API will stop at the first appearance of a stop\n     * sequence.\n     *\n     * Note: The stop sequence will not be included as part of the response.\n     * Note: stopSequences is only supported for Gemini models\n     */\n    stopSequences?: string[];\n    /**\n     * A list of unique `SafetySetting` instances for blocking unsafe content. The API will block\n     * any prompts and responses that fail to meet the thresholds set by these settings. If there\n     * is no `SafetySetting` for a given `SafetyCategory` provided in the list, the API will use\n     * the default safety setting for that category.\n     */\n    safetySettings?: SafetySetting[];\n    /**\n     * Google API key to use\n     */\n    apiKey?: string;\n    /**\n     * Google API version to use\n     */\n    apiVersion?: string;\n    /**\n     * Google API base URL to use\n     */\n    baseUrl?: string;\n    /**\n     * Google API custom headers to use\n     */\n    customHeaders?: Record<string, string>;\n    /** Whether to stream the results or not */\n    streaming?: boolean;\n    /**\n     * Whether or not to force the model to respond with JSON.\n     * Available for `gemini-1.5` models and later.\n     * @default false\n     */\n    json?: boolean;\n    /**\n     * Whether or not model supports system instructions.\n     * The following models support system instructions:\n     * - All Gemini 1.5 Pro model versions\n     * - All Gemini 1.5 Flash model versions\n     * - Gemini 1.0 Pro version gemini-1.0-pro-002\n     */\n    convertSystemMessageToHumanContent?: boolean | undefined;\n    /**\n     * Optional. Config for thinking features. An error will be returned if this\n     * field is set for models that don't support thinking.\n     */\n    thinkingConfig?: GoogleGenerativeAIThinkingConfig;\n}\n/**\n * Google Generative AI chat model integration.\n *\n * Setup:\n * Install `@langchain/google-genai` and set an environment variable named `GOOGLE_API_KEY`.\n *\n * ```bash\n * npm install @langchain/google-genai\n * export GOOGLE_API_KEY=\"your-api-key\"\n * ```\n *\n * ## [Constructor args](https://api.js.langchain.com/classes/langchain_google_genai.ChatGoogleGenerativeAI.html#constructor)\n *\n * ## [Runtime args](https://api.js.langchain.com/interfaces/langchain_google_genai.GoogleGenerativeAIChatCallOptions.html)\n *\n * Runtime args can be passed as the second argument to any of the base runnable methods `.invoke`. `.stream`, `.batch`, etc.\n * They can also be passed via `.withConfig`, or the second arg in `.bindTools`, like shown in the examples below:\n *\n * ```typescript\n * // When calling `.withConfig`, call options should be passed via the first argument\n * const llmWithArgsBound = llm.withConfig({\n *   stop: [\"\\n\"],\n * });\n *\n * // When calling `.bindTools`, call options should be passed via the second argument\n * const llmWithTools = llm.bindTools(\n *   [...],\n *   {\n *     stop: [\"\\n\"],\n *   }\n * );\n * ```\n *\n * ## Examples\n *\n * <details open>\n * <summary><strong>Instantiate</strong></summary>\n *\n * ```typescript\n * import { ChatGoogleGenerativeAI } from '@langchain/google-genai';\n *\n * const llm = new ChatGoogleGenerativeAI({\n *   model: \"gemini-1.5-flash\",\n *   temperature: 0,\n *   maxRetries: 2,\n *   // apiKey: \"...\",\n *   // other params...\n * });\n * ```\n * </details>\n *\n * <br />\n *\n * <details>\n * <summary><strong>Invoking</strong></summary>\n *\n * ```typescript\n * const input = `Translate \"I love programming\" into French.`;\n *\n * // Models also accept a list of chat messages or a formatted prompt\n * const result = await llm.invoke(input);\n * console.log(result);\n * ```\n *\n * ```txt\n * AIMessage {\n *   \"content\": \"There are a few ways to translate \\\"I love programming\\\" into French, depending on the level of formality and nuance you want to convey:\\n\\n**Formal:**\\n\\n* **J'aime la programmation.** (This is the most literal and formal translation.)\\n\\n**Informal:**\\n\\n* **J'adore programmer.** (This is a more enthusiastic and informal translation.)\\n* **J'aime beaucoup programmer.** (This is a slightly less enthusiastic but still informal translation.)\\n\\n**More specific:**\\n\\n* **J'aime beaucoup coder.** (This specifically refers to writing code.)\\n* **J'aime beaucoup développer des logiciels.** (This specifically refers to developing software.)\\n\\nThe best translation will depend on the context and your intended audience. \\n\",\n *   \"response_metadata\": {\n *     \"finishReason\": \"STOP\",\n *     \"index\": 0,\n *     \"safetyRatings\": [\n *       {\n *         \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n *         \"probability\": \"NEGLIGIBLE\"\n *       },\n *       {\n *         \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n *         \"probability\": \"NEGLIGIBLE\"\n *       },\n *       {\n *         \"category\": \"HARM_CATEGORY_HARASSMENT\",\n *         \"probability\": \"NEGLIGIBLE\"\n *       },\n *       {\n *         \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n *         \"probability\": \"NEGLIGIBLE\"\n *       }\n *     ]\n *   },\n *   \"usage_metadata\": {\n *     \"input_tokens\": 10,\n *     \"output_tokens\": 149,\n *     \"total_tokens\": 159\n *   }\n * }\n * ```\n * </details>\n *\n * <br />\n *\n * <details>\n * <summary><strong>Streaming Chunks</strong></summary>\n *\n * ```typescript\n * for await (const chunk of await llm.stream(input)) {\n *   console.log(chunk);\n * }\n * ```\n *\n * ```txt\n * AIMessageChunk {\n *   \"content\": \"There\",\n *   \"response_metadata\": {\n *     \"index\": 0\n *   }\n *   \"usage_metadata\": {\n *     \"input_tokens\": 10,\n *     \"output_tokens\": 1,\n *     \"total_tokens\": 11\n *   }\n * }\n * AIMessageChunk {\n *   \"content\": \" are a few ways to translate \\\"I love programming\\\" into French, depending on\",\n * }\n * AIMessageChunk {\n *   \"content\": \" the level of formality and nuance you want to convey:\\n\\n**Formal:**\\n\\n\",\n * }\n * AIMessageChunk {\n *   \"content\": \"* **J'aime la programmation.** (This is the most literal and formal translation.)\\n\\n**Informal:**\\n\\n* **J'adore programmer.** (This\",\n * }\n * AIMessageChunk {\n *   \"content\": \" is a more enthusiastic and informal translation.)\\n* **J'aime beaucoup programmer.** (This is a slightly less enthusiastic but still informal translation.)\\n\\n**More\",\n * }\n * AIMessageChunk {\n *   \"content\": \" specific:**\\n\\n* **J'aime beaucoup coder.** (This specifically refers to writing code.)\\n* **J'aime beaucoup développer des logiciels.** (This specifically refers to developing software.)\\n\\nThe best translation will depend on the context and\",\n * }\n * AIMessageChunk {\n *   \"content\": \" your intended audience. \\n\",\n * }\n * ```\n * </details>\n *\n * <br />\n *\n * <details>\n * <summary><strong>Aggregate Streamed Chunks</strong></summary>\n *\n * ```typescript\n * import { AIMessageChunk } from '@langchain/core/messages';\n * import { concat } from '@langchain/core/utils/stream';\n *\n * const stream = await llm.stream(input);\n * let full: AIMessageChunk | undefined;\n * for await (const chunk of stream) {\n *   full = !full ? chunk : concat(full, chunk);\n * }\n * console.log(full);\n * ```\n *\n * ```txt\n * AIMessageChunk {\n *   \"content\": \"There are a few ways to translate \\\"I love programming\\\" into French, depending on the level of formality and nuance you want to convey:\\n\\n**Formal:**\\n\\n* **J'aime la programmation.** (This is the most literal and formal translation.)\\n\\n**Informal:**\\n\\n* **J'adore programmer.** (This is a more enthusiastic and informal translation.)\\n* **J'aime beaucoup programmer.** (This is a slightly less enthusiastic but still informal translation.)\\n\\n**More specific:**\\n\\n* **J'aime beaucoup coder.** (This specifically refers to writing code.)\\n* **J'aime beaucoup développer des logiciels.** (This specifically refers to developing software.)\\n\\nThe best translation will depend on the context and your intended audience. \\n\",\n *   \"usage_metadata\": {\n *     \"input_tokens\": 10,\n *     \"output_tokens\": 277,\n *     \"total_tokens\": 287\n *   }\n * }\n * ```\n * </details>\n *\n * <br />\n *\n * <details>\n * <summary><strong>Bind tools</strong></summary>\n *\n * ```typescript\n * import { z } from 'zod';\n *\n * const GetWeather = {\n *   name: \"GetWeather\",\n *   description: \"Get the current weather in a given location\",\n *   schema: z.object({\n *     location: z.string().describe(\"The city and state, e.g. San Francisco, CA\")\n *   }),\n * }\n *\n * const GetPopulation = {\n *   name: \"GetPopulation\",\n *   description: \"Get the current population in a given location\",\n *   schema: z.object({\n *     location: z.string().describe(\"The city and state, e.g. San Francisco, CA\")\n *   }),\n * }\n *\n * const llmWithTools = llm.bindTools([GetWeather, GetPopulation]);\n * const aiMsg = await llmWithTools.invoke(\n *   \"Which city is hotter today and which is bigger: LA or NY?\"\n * );\n * console.log(aiMsg.tool_calls);\n * ```\n *\n * ```txt\n * [\n *   {\n *     name: 'GetWeather',\n *     args: { location: 'Los Angeles, CA' },\n *     type: 'tool_call'\n *   },\n *   {\n *     name: 'GetWeather',\n *     args: { location: 'New York, NY' },\n *     type: 'tool_call'\n *   },\n *   {\n *     name: 'GetPopulation',\n *     args: { location: 'Los Angeles, CA' },\n *     type: 'tool_call'\n *   },\n *   {\n *     name: 'GetPopulation',\n *     args: { location: 'New York, NY' },\n *     type: 'tool_call'\n *   }\n * ]\n * ```\n * </details>\n *\n * <br />\n *\n * <details>\n * <summary><strong>Structured Output</strong></summary>\n *\n * ```typescript\n * const Joke = z.object({\n *   setup: z.string().describe(\"The setup of the joke\"),\n *   punchline: z.string().describe(\"The punchline to the joke\"),\n *   rating: z.number().optional().describe(\"How funny the joke is, from 1 to 10\")\n * }).describe('Joke to tell user.');\n *\n * const structuredLlm = llm.withStructuredOutput(Joke, { name: \"Joke\" });\n * const jokeResult = await structuredLlm.invoke(\"Tell me a joke about cats\");\n * console.log(jokeResult);\n * ```\n *\n * ```txt\n * {\n *   setup: \"Why don\\\\'t cats play poker?\",\n *   punchline: \"Why don\\\\'t cats play poker? Because they always have an ace up their sleeve!\"\n * }\n * ```\n * </details>\n *\n * <br />\n *\n * <details>\n * <summary><strong>Multimodal</strong></summary>\n *\n * ```typescript\n * import { HumanMessage } from '@langchain/core/messages';\n *\n * const imageUrl = \"https://example.com/image.jpg\";\n * const imageData = await fetch(imageUrl).then(res => res.arrayBuffer());\n * const base64Image = Buffer.from(imageData).toString('base64');\n *\n * const message = new HumanMessage({\n *   content: [\n *     { type: \"text\", text: \"describe the weather in this image\" },\n *     {\n *       type: \"image_url\",\n *       image_url: { url: `data:image/jpeg;base64,${base64Image}` },\n *     },\n *   ]\n * });\n *\n * const imageDescriptionAiMsg = await llm.invoke([message]);\n * console.log(imageDescriptionAiMsg.content);\n * ```\n *\n * ```txt\n * The weather in the image appears to be clear and sunny. The sky is mostly blue with a few scattered white clouds, indicating fair weather. The bright sunlight is casting shadows on the green, grassy hill, suggesting it is a pleasant day with good visibility. There are no signs of rain or stormy conditions.\n * ```\n * </details>\n *\n * <br />\n *\n * <details>\n * <summary><strong>Usage Metadata</strong></summary>\n *\n * ```typescript\n * const aiMsgForMetadata = await llm.invoke(input);\n * console.log(aiMsgForMetadata.usage_metadata);\n * ```\n *\n * ```txt\n * { input_tokens: 10, output_tokens: 149, total_tokens: 159 }\n * ```\n * </details>\n *\n * <br />\n *\n * <details>\n * <summary><strong>Response Metadata</strong></summary>\n *\n * ```typescript\n * const aiMsgForResponseMetadata = await llm.invoke(input);\n * console.log(aiMsgForResponseMetadata.response_metadata);\n * ```\n *\n * ```txt\n * {\n *   finishReason: 'STOP',\n *   index: 0,\n *   safetyRatings: [\n *     {\n *       category: 'HARM_CATEGORY_SEXUALLY_EXPLICIT',\n *       probability: 'NEGLIGIBLE'\n *     },\n *     {\n *       category: 'HARM_CATEGORY_HATE_SPEECH',\n *       probability: 'NEGLIGIBLE'\n *     },\n *     { category: 'HARM_CATEGORY_HARASSMENT', probability: 'NEGLIGIBLE' },\n *     {\n *       category: 'HARM_CATEGORY_DANGEROUS_CONTENT',\n *       probability: 'NEGLIGIBLE'\n *     }\n *   ]\n * }\n * ```\n * </details>\n *\n * <br />\n *\n * <details>\n * <summary><strong>Document Messages</strong></summary>\n *\n * This example will show you how to pass documents such as PDFs to Google\n * Generative AI through messages.\n *\n * ```typescript\n * const pdfPath = \"/Users/my_user/Downloads/invoice.pdf\";\n * const pdfBase64 = await fs.readFile(pdfPath, \"base64\");\n *\n * const response = await llm.invoke([\n *   [\"system\", \"Use the provided documents to answer the question\"],\n *   [\n *     \"user\",\n *     [\n *       {\n *         type: \"application/pdf\", // If the `type` field includes a single slash (`/`), it will be treated as inline data.\n *         data: pdfBase64,\n *       },\n *       {\n *         type: \"text\",\n *         text: \"Summarize the contents of this PDF\",\n *       },\n *     ],\n *   ],\n * ]);\n *\n * console.log(response.content);\n * ```\n *\n * ```txt\n * This is a billing invoice from Twitter Developers for X API Basic Access. The transaction date is January 7, 2025,\n * and the amount is $194.34, which has been paid. The subscription period is from January 7, 2025 21:02 to February 7, 2025 00:00 (UTC).\n * The tax is $0.00, with a tax rate of 0%. The total amount is $194.34. The payment was made using a Visa card ending in 7022,\n * expiring in 12/2026. The billing address is Brace Sproul, 1234 Main Street, San Francisco, CA, US 94103. The company being billed is\n * X Corp, located at 865 FM 1209 Building 2, Bastrop, TX, US 78602. Terms and conditions apply.\n * ```\n * </details>\n *\n * <br />\n */\nexport declare class ChatGoogleGenerativeAI extends BaseChatModel<GoogleGenerativeAIChatCallOptions, AIMessageChunk> implements GoogleGenerativeAIChatInput {\n    static lc_name(): string;\n    lc_serializable: boolean;\n    get lc_secrets(): {\n        [key: string]: string;\n    } | undefined;\n    lc_namespace: string[];\n    get lc_aliases(): {\n        apiKey: string;\n    };\n    model: string;\n    temperature?: number;\n    maxOutputTokens?: number;\n    topP?: number;\n    topK?: number;\n    stopSequences: string[];\n    safetySettings?: SafetySetting[];\n    apiKey?: string;\n    streaming: boolean;\n    json?: boolean;\n    streamUsage: boolean;\n    convertSystemMessageToHumanContent: boolean | undefined;\n    thinkingConfig?: GoogleGenerativeAIThinkingConfig;\n    private client;\n    get _isMultimodalModel(): boolean;\n    constructor(fields: GoogleGenerativeAIChatInput);\n    useCachedContent(cachedContent: CachedContent, modelParams?: ModelParams, requestOptions?: RequestOptions): void;\n    get useSystemInstruction(): boolean;\n    get computeUseSystemInstruction(): boolean;\n    getLsParams(options: this[\"ParsedCallOptions\"]): LangSmithParams;\n    _combineLLMOutput(): never[];\n    _llmType(): string;\n    bindTools(tools: GoogleGenerativeAIToolType[], kwargs?: Partial<GoogleGenerativeAIChatCallOptions>): Runnable<BaseLanguageModelInput, AIMessageChunk, GoogleGenerativeAIChatCallOptions>;\n    invocationParams(options?: this[\"ParsedCallOptions\"]): Omit<GenerateContentRequest, \"contents\">;\n    _generate(messages: BaseMessage[], options: this[\"ParsedCallOptions\"], runManager?: CallbackManagerForLLMRun): Promise<ChatResult>;\n    _streamResponseChunks(messages: BaseMessage[], options: this[\"ParsedCallOptions\"], runManager?: CallbackManagerForLLMRun): AsyncGenerator<ChatGenerationChunk>;\n    completionWithRetry(request: string | GenerateContentRequest | (string | GenerativeAIPart)[], options?: this[\"ParsedCallOptions\"]): Promise<import(\"@google/generative-ai\").GenerateContentResult>;\n    /**\n     * Return profiling information for the model.\n     *\n     * Provides information about the model's capabilities and constraints,\n     * including token limits, multimodal support, and advanced features like\n     * tool calling and structured output.\n     *\n     * @returns {ModelProfile} An object describing the model's capabilities and constraints\n     *\n     * @example\n     * ```typescript\n     * const model = new ChatGoogleGenerativeAI({ model: \"gemini-1.5-flash\" });\n     * const profile = model.profile;\n     * console.log(profile.maxInputTokens); // 2000000\n     * console.log(profile.imageInputs); // true\n     * ```\n     */\n    get profile(): ModelProfile;\n    withStructuredOutput<RunOutput extends Record<string, any> = Record<string, any>>(outputSchema: InteropZodType<RunOutput> | Record<string, any>, config?: StructuredOutputMethodOptions<false>): Runnable<BaseLanguageModelInput, RunOutput>;\n    withStructuredOutput<RunOutput extends Record<string, any> = Record<string, any>>(outputSchema: InteropZodType<RunOutput> | Record<string, any>, config?: StructuredOutputMethodOptions<true>): Runnable<BaseLanguageModelInput, {\n        raw: BaseMessage;\n        parsed: RunOutput;\n    }>;\n}\n//# sourceMappingURL=chat_models.d.ts.map"],"mappings":";;;;;;;;;;;;;KAUYwB,sBAAAA;SACDd;UACCA;AAFZ,CAAA;AAIiBe,UAAAA,iCAAAA,SAA0CX,wBAAT,CAAA;EACtCS,KAAAA,CAAAA,EAAAA,0BAAAA,EAAAA;EAeShB;;AAhB8D;AAqBnF;EAA+EkB,oBAAAA,CAAAA,EAAAA,MAAAA,EAAAA;EA6D1DxB;;;;;EA7DyD,WAAA,CAAA,EAAA,OAAA;EA2dzD4B;;;EAgBA5B,cAAAA,CAAAA,EAhfAM,MAgfAN;;;;;AAU0EI,UArf9EqB,2BAAAA,SAAoCV,mBAqf0CX,EArfrBuB,IAqfqBvB,CArfhBoB,iCAqfgBpB,EAAAA,aAAAA,CAAAA,CAAAA;EAG1CU;;;;;EAGqFN,KAAAA,EAAAA,MAAAA;EAAgBgB;;;;;;;;;;EAGZd,WAAAA,CAAAA,EAAAA,MAAAA;EAAfuB;;;EAClCC,eAAAA,CAAAA,EAAAA,MAAmFC;EAAxCH;;;;;;;;;;;;EAoBvEN,IAAAA,CAAAA,EAAAA,MAAAA;EAAkDG;;;;;;;;;;EAxDwC,IAAA,CAAA,EAAA,MAAA;;;;;;;;;;;;;;;;mBA9ZtI7B;;;;;;;;;;;;;;;;kBAgBD0B;;;;;;;;;;;;;;;;;;;;;mBAqBCL;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;cAyXAO,sBAAAA,SAA+BhB,cAAcY,mCAAmChB,2BAA2BiB;;;;;;;;;;;;;;;;mBAgB3GzB;;;;;;mBAMAqB;;;sBAGGI;kCACYpB,6BAA6BF,8BAA8BC;;;mDAG1CU;;;mBAGhCQ,uCAAuCQ,QAAQN,qCAAqCL,SAASF,wBAAwBT,gBAAgBgB;yDAC/FO,KAAKhC;sBACxCU,gEAAgEF,2BAA2ByB,QAAQrB;kCACvFF,gEAAgEF,2BAA2B0B,eAAevB;wCACpGX,mCAAmCG,+CAA2D8B,QAA3CE,sBAAAA,CAAmFC,qBAAAA;;;;;;;;;;;;;;;;;;iBAkB7JnB;yCACwBU,sBAAsBA,mCAAmCN,eAAeS,aAAaH,8BAA8BR,uCAAuCC,SAASF,wBAAwBY;yCAC3LH,sBAAsBA,mCAAmCN,eAAeS,aAAaH,8BAA8BR,sCAAsCC,SAASF;SAChMR;YACGoB"}