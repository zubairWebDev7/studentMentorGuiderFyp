{"version":3,"file":"index.d.cts","names":["AwsCredentialIdentity","Provider","BaseMessage","StructuredToolInterface","ChatGeneration","CredentialType","BaseBedrockInput","fetch","Record","Dict","BedrockLLMInputOutputAdapter"],"sources":["../../../src/utils/bedrock/index.d.ts"],"sourcesContent":["import type { AwsCredentialIdentity, Provider } from \"@aws-sdk/types\";\nimport { BaseMessage } from \"@langchain/core/messages\";\nimport { StructuredToolInterface } from \"@langchain/core/tools\";\nimport { ChatGeneration } from \"@langchain/core/outputs\";\nexport type CredentialType = AwsCredentialIdentity | Provider<AwsCredentialIdentity>;\n/** Bedrock models.\n    To authenticate, the AWS client uses the following methods to automatically load credentials:\n    https://boto3.amazonaws.com/v1/documentation/api/latest/guide/credentials.html\n    If a specific credential profile should be used, you must pass the name of the profile from the ~/.aws/credentials file that is to be used.\n    Make sure the credentials / roles used have the required policies to access the Bedrock service.\n*/\nexport interface BaseBedrockInput {\n    /** Model to use.\n        For example, \"amazon.titan-tg1-large\", this is equivalent to the modelId property in the list-foundation-models api.\n    */\n    model: string;\n    /** Optional URL Encoded overide for URL model parameter in fetch. Necessary for invoking an Application Inference Profile.\n        For example, \"arn%3Aaws%3Abedrock%3Aus-east-1%3A1234567890%3Aapplication-inference-profile%2Fabcdefghi\", will override this.model in final /invoke URL call.\n        Must still provide `model` as normal modelId to benefit from all the metadata.\n    */\n    applicationInferenceProfile?: string;\n    /** The AWS region e.g. `us-west-2`.\n        Fallback to AWS_DEFAULT_REGION env variable or region specified in ~/.aws/config in case it is not provided here.\n    */\n    region?: string;\n    /** AWS Credentials.\n        If no credentials are provided, the default credentials from `@aws-sdk/credential-provider-node` will be used.\n     */\n    credentials?: CredentialType;\n    /** Temperature. */\n    temperature?: number;\n    /** Max tokens. */\n    maxTokens?: number;\n    /** A custom fetch function for low-level access to AWS API. Defaults to fetch(). */\n    fetchFn?: typeof fetch;\n    /** Override the default endpoint hostname. */\n    endpointHost?: string;\n    /** Additional kwargs to pass to the model. */\n    modelKwargs?: Record<string, unknown>;\n    /** Whether or not to stream responses */\n    streaming: boolean;\n    /** Trace settings for the Bedrock Guardrails. */\n    trace?: \"ENABLED\" | \"DISABLED\";\n    /** Identifier for the guardrail configuration. */\n    guardrailIdentifier?: string;\n    /** Version for the guardrail configuration. */\n    guardrailVersion?: string;\n    /** Required when Guardrail is in use. */\n    guardrailConfig?: {\n        tagSuffix: string;\n        streamProcessingMode: \"SYNCHRONOUS\" | \"ASYNCHRONOUS\";\n    };\n    awsAccessKeyId?: string;\n    awsSecretAccessKey?: string;\n    awsSessionToken?: string;\n}\ntype Dict = {\n    [key: string]: unknown;\n};\n/**\n * A helper class used within the `Bedrock` class. It is responsible for\n * preparing the input and output for the Bedrock service. It formats the\n * input prompt based on the provider (e.g., \"anthropic\", \"ai21\",\n * \"amazon\") and extracts the generated text from the service response.\n */\nexport declare class BedrockLLMInputOutputAdapter {\n    /** Adapter class to prepare the inputs from Langchain to a format\n    that LLM model expects. Also, provides a helper function to extract\n    the generated text from the model response. */\n    static prepareInput(provider: string, prompt: string, maxTokens?: number, temperature?: number, stopSequences?: string[] | undefined, modelKwargs?: Record<string, unknown>, bedrockMethod?: \"invoke\" | \"invoke-with-response-stream\", guardrailConfig?: {\n        tagSuffix: string;\n        streamProcessingMode: \"SYNCHRONOUS\" | \"ASYNCHRONOUS\";\n    } | undefined): Dict;\n    static prepareMessagesInput(provider: string, messages: BaseMessage[], maxTokens?: number, temperature?: number, stopSequences?: string[] | undefined, modelKwargs?: Record<string, unknown>, guardrailConfig?: {\n        tagSuffix: string;\n        streamProcessingMode: \"SYNCHRONOUS\" | \"ASYNCHRONOUS\";\n    } | undefined, tools?: (StructuredToolInterface | Record<string, unknown>)[]): Dict;\n    /**\n     * Extracts the generated text from the service response.\n     * @param provider The provider name.\n     * @param responseBody The response body from the service.\n     * @returns The generated text.\n     */\n    static prepareOutput(provider: string, responseBody: any): string;\n    static prepareMessagesOutput(provider: string, response: any, fields?: {\n        coerceContentToString?: boolean;\n    }): ChatGeneration | undefined;\n}\nexport {};\n//# sourceMappingURL=index.d.ts.map"],"mappings":";;;;;KAIYK,cAAAA,GAAiBL,wBAAwBC,SAASD;AAA9D;;;;AAA6D;AAO7D;AAiBkBK,UAjBDC,gBAAAA,CAiBCD;EAMGE;;AAIG;;;;;;;;;;;;;;gBAVNF;;;;;;mBAMGE;;;;gBAIHC"}