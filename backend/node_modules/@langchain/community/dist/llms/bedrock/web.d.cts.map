{"version":3,"file":"web.d.cts","names":["EventStreamCodec","CallbackManagerForLLMRun","GenerationChunk","LLM","BaseLLMParams","BaseBedrockInput","CredentialType","SerializedFields","Bedrock","fetch","Record","Partial","Promise","Response","AsyncGenerator","ArrayBuffer","Uint8Array"],"sources":["../../../src/llms/bedrock/web.d.ts"],"sourcesContent":["import { EventStreamCodec } from \"@smithy/eventstream-codec\";\nimport { CallbackManagerForLLMRun } from \"@langchain/core/callbacks/manager\";\nimport { GenerationChunk } from \"@langchain/core/outputs\";\nimport { LLM, type BaseLLMParams } from \"@langchain/core/language_models/llms\";\nimport { BaseBedrockInput, type CredentialType } from \"../../utils/bedrock/index.js\";\nimport type { SerializedFields } from \"../../load/map_keys.js\";\n/**\n * A type of Large Language Model (LLM) that interacts with the Bedrock\n * service. It extends the base `LLM` class and implements the\n * `BaseBedrockInput` interface. The class is designed to authenticate and\n * interact with the Bedrock service, which is a part of Amazon Web\n * Services (AWS). It uses AWS credentials for authentication and can be\n * configured with various parameters such as the model to use, the AWS\n * region, and the maximum number of tokens to generate.\n */\nexport declare class Bedrock extends LLM implements BaseBedrockInput {\n    model: string;\n    modelProvider: string;\n    region: string;\n    credentials: CredentialType;\n    temperature?: number | undefined;\n    maxTokens?: number | undefined;\n    fetchFn: typeof fetch;\n    endpointHost?: string;\n    modelKwargs?: Record<string, unknown>;\n    codec: EventStreamCodec;\n    streaming: boolean;\n    lc_serializable: boolean;\n    get lc_aliases(): Record<string, string>;\n    get lc_secrets(): {\n        [key: string]: string;\n    } | undefined;\n    get lc_attributes(): SerializedFields | undefined;\n    _llmType(): string;\n    static lc_name(): string;\n    constructor(fields?: Partial<BaseBedrockInput> & BaseLLMParams);\n    /** Call out to Bedrock service model.\n      Arguments:\n        prompt: The prompt to pass into the model.\n  \n      Returns:\n        The string generated by the model.\n  \n      Example:\n        response = model.invoke(\"Tell me a joke.\")\n    */\n    _call(prompt: string, options: this[\"ParsedCallOptions\"], runManager?: CallbackManagerForLLMRun): Promise<string>;\n    _signedFetch(prompt: string, options: this[\"ParsedCallOptions\"], fields: {\n        bedrockMethod: \"invoke\" | \"invoke-with-response-stream\";\n        endpointHost: string;\n        provider: string;\n    }): Promise<Response>;\n    invocationParams(options?: this[\"ParsedCallOptions\"]): {\n        model: string;\n        region: string;\n        temperature: number | undefined;\n        maxTokens: number | undefined;\n        stop: string[] | undefined;\n        modelKwargs: Record<string, unknown> | undefined;\n    };\n    _streamResponseChunks(prompt: string, options: this[\"ParsedCallOptions\"], runManager?: CallbackManagerForLLMRun): AsyncGenerator<GenerationChunk>;\n    _readChunks(reader: any): {\n        [Symbol.asyncIterator](): AsyncGenerator<Uint8Array<ArrayBuffer>, void, unknown>;\n    };\n}\n//# sourceMappingURL=web.d.ts.map"],"mappings":";;;;;;;;;;;AAeA;;;;;;;AAoBiCK,cApBZG,OAAAA,SAAgBL,GAAAA,YAAeE,gBAoBnBA,CAAAA;EAARM,KAAAA,EAAAA,MAAAA;EAA4BP,aAAAA,EAAAA,MAAAA;EAWsBH,MAAAA,EAAAA,MAAAA;EAA2BW,WAAAA,EA3BrFN,cA2BqFM;EAKtFC,WAAAA,CAAAA,EAAAA,MAAAA,GAAAA,SAAAA;EAARD,SAAAA,CAAAA,EAAAA,MAAAA,GAAAA,SAAAA;EAOaF,OAAAA,EAAAA,OApCDD,KAoCCC;EAEsET,YAAAA,CAAAA,EAAAA,MAAAA;EAA0CC,WAAAA,CAAAA,EApCnHQ,MAoCmHR,CAAAA,MAAAA,EAAAA,OAAAA,CAAAA;EAAfY,KAAAA,EAnC3Gd,gBAmC2Gc;EAE1DC,SAAAA,EAAAA,OAAAA;EAAXC,eAAAA,EAAAA,OAAAA;EAAfF,IAAAA,UAAAA,CAAAA,CAAAA,EAlCZJ,MAkCYI,CAAAA,MAAAA,EAAAA,MAAAA,CAAAA;EA/CGX,IAAAA,UAAAA,CAAAA,CAAAA,EAAAA;IAAeE,CAAAA,GAAAA,EAAAA,MAAAA,CAAAA,EAAAA,MAAAA;EAAgB,CAAA,GAAA,SAAA;uBAiB3CE;;;uBAGAI,QAAQN,oBAAoBD;;;;;;;;;yEAWsBH,2BAA2BW;;;;;MAK9FA,QAAQC;;;;;;;iBAOKH;;yFAEsET,2BAA2Ba,eAAeZ;;8BAEnGY,eAAeE,WAAWD"}