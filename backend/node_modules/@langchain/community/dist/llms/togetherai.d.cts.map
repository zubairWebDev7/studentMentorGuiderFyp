{"version":3,"file":"togetherai.d.cts","names":["CallbackManagerForLLMRun","LLM","BaseLLMCallOptions","BaseLLMParams","GenerationChunk","TogetherAIInputs","TogetherAICallOptions","Pick","TogetherAI","Promise","AsyncGenerator"],"sources":["../../src/llms/togetherai.d.ts"],"sourcesContent":["import { CallbackManagerForLLMRun } from \"@langchain/core/callbacks/manager\";\nimport { LLM, type BaseLLMCallOptions, type BaseLLMParams } from \"@langchain/core/language_models/llms\";\nimport { GenerationChunk } from \"@langchain/core/outputs\";\n/**\n * Note that the modelPath is the only required parameter. For testing you\n * can set this in the environment variable `LLAMA_PATH`.\n */\nexport interface TogetherAIInputs extends BaseLLMParams {\n    /**\n     * The API key to use for the TogetherAI API.\n     * @default {process.env.TOGETHER_AI_API_KEY}\n     */\n    apiKey?: string;\n    /**\n     * The name of the model to query.\n     * Alias for `model`\n     */\n    modelName?: string;\n    /**\n     * The name of the model to query.\n     */\n    model?: string;\n    /**\n     * A decimal number that determines the degree of randomness in the response.\n     * A value of 1 will always yield the same output.\n     * A temperature less than 1 favors more correctness and is appropriate for question answering or summarization.\n     * A value greater than 1 introduces more randomness in the output.\n     * @default {0.7}\n     */\n    temperature?: number;\n    /**\n     * Whether or not to stream tokens as they are generated.\n     * @default {false}\n     */\n    streaming?: boolean;\n    /**\n     * The `topP` (nucleus) parameter is used to dynamically adjust the number of choices for each predicted token based on the cumulative probabilities.\n     * It specifies a probability threshold, below which all less likely tokens are filtered out.\n     * This technique helps to maintain diversity and generate more fluent and natural-sounding text.\n     * @default {0.7}\n     */\n    topP?: number;\n    /**\n     * The `topK` parameter is used to limit the number of choices for the next predicted word or token.\n     * It specifies the maximum number of tokens to consider at each step, based on their probability of occurrence.\n     * This technique helps to speed up the generation process and can improve the quality of the generated text by focusing on the most likely options.\n     * @default {50}\n     */\n    topK?: number;\n    /**\n     * A number that controls the diversity of generated text by reducing the likelihood of repeated sequences.\n     * Higher values decrease repetition.\n     * @default {1}\n     */\n    repetitionPenalty?: number;\n    /**\n     * An integer that specifies how many top token log probabilities are included in the response for each token generation step.\n     */\n    logprobs?: number;\n    /**\n     * Run an LLM-based input-output safeguard model on top of any model.\n     */\n    safetyModel?: string;\n    /**\n     * Limit the number of tokens generated.\n     */\n    maxTokens?: number;\n    /**\n     * A list of tokens at which the generation should stop.\n     */\n    stop?: string[];\n}\nexport interface TogetherAICallOptions extends BaseLLMCallOptions, Pick<TogetherAIInputs, \"modelName\" | \"model\" | \"temperature\" | \"topP\" | \"topK\" | \"repetitionPenalty\" | \"logprobs\" | \"safetyModel\" | \"maxTokens\" | \"stop\"> {\n}\nexport declare class TogetherAI extends LLM<TogetherAICallOptions> {\n    lc_serializable: boolean;\n    static inputs: TogetherAIInputs;\n    temperature: number;\n    topP: number;\n    topK: number;\n    modelName: string;\n    model: string;\n    streaming: boolean;\n    repetitionPenalty: number;\n    logprobs?: number;\n    maxTokens?: number;\n    safetyModel?: string;\n    stop?: string[];\n    private apiKey;\n    private inferenceAPIUrl;\n    static lc_name(): string;\n    /**\n     * Check if a model name appears to be a chat/instruct model\n     * @param modelName The model name to check\n     * @returns true if the model appears to be a chat/instruct model\n     */\n    private isChatModel;\n    constructor(inputs: TogetherAIInputs);\n    _llmType(): string;\n    private constructHeaders;\n    private constructBody;\n    completionWithRetry(prompt: string, options?: this[\"ParsedCallOptions\"]): Promise<any>;\n    /** @ignore */\n    _call(prompt: string, options?: this[\"ParsedCallOptions\"]): Promise<string>;\n    _streamResponseChunks(prompt: string, options: this[\"ParsedCallOptions\"], runManager?: CallbackManagerForLLMRun): AsyncGenerator<GenerationChunk>;\n}\n//# sourceMappingURL=togetherai.d.ts.map"],"mappings":";;;;;;;;AAOA;AAiEA;AAAwEK,UAjEvDA,gBAAAA,SAAyBF,aAiE8BE,CAAAA;EAAzBH;;AAAwB;AAEvE;EAA4CI,MAAAA,CAAAA,EAAAA,MAAAA;EAEzBD;;;;EA4BwEL,SAAAA,CAAAA,EAAAA,MAAAA;EAA0CI;;;EA9B1F,KAAA,CAAA,EAAA,MAAA;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;UAF1BE,qBAAAA,SAA8BJ,oBAAoBK,KAAKF;cAEnDG,UAAAA,SAAmBP,IAAIK;;iBAEzBD;;;;;;;;;;;;;;;;;;;;;sBAqBKA;;;;4EAIsDI;;8DAEdA;yFAC2BT,2BAA2BU,eAAeN"}