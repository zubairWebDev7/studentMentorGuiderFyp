{"version":3,"file":"llm_chain.cjs","names":["llmLike: unknown","llmLike: RunnableInterface","Runnable","BaseChain","fields: LLMChainInput<T, Model>","NoOpOutputParser","values: ChainValues","generations: Generation[]","promptValue: BasePromptValueInterface","runManager?: CallbackManagerForChainRun","finalCompletion: unknown","values: ChainValues & CallOptionsIfAvailable<Model>","config?: Callbacks | BaseCallbackConfig","callbackManager?: CallbackManager","data: SerializedLLMChain","BaseLanguageModel","BasePromptTemplate","text: string"],"sources":["../../src/chains/llm_chain.ts"],"sourcesContent":["import {\n  BaseLanguageModel,\n  BaseLanguageModelInterface,\n  BaseLanguageModelInput,\n} from \"@langchain/core/language_models/base\";\nimport type { ChainValues } from \"@langchain/core/utils/types\";\nimport type { Generation } from \"@langchain/core/outputs\";\nimport type { BaseMessage } from \"@langchain/core/messages\";\nimport type { BasePromptValueInterface } from \"@langchain/core/prompt_values\";\nimport { BasePromptTemplate } from \"@langchain/core/prompts\";\nimport {\n  BaseLLMOutputParser,\n  BaseOutputParser,\n} from \"@langchain/core/output_parsers\";\nimport {\n  CallbackManager,\n  BaseCallbackConfig,\n  CallbackManagerForChainRun,\n  Callbacks,\n} from \"@langchain/core/callbacks/manager\";\nimport { Runnable, type RunnableInterface } from \"@langchain/core/runnables\";\nimport { BaseChain, ChainInputs } from \"./base.js\";\nimport { SerializedLLMChain } from \"./serde.js\";\nimport { NoOpOutputParser } from \"../output_parsers/noop.js\";\n\ntype LLMType =\n  | BaseLanguageModelInterface\n  | Runnable<BaseLanguageModelInput, string>\n  | Runnable<BaseLanguageModelInput, BaseMessage>;\n\n// eslint-disable-next-line @typescript-eslint/no-explicit-any\ntype CallOptionsIfAvailable<T> = T extends { CallOptions: infer CO } ? CO : any;\n/**\n * Interface for the input parameters of the LLMChain class.\n */\nexport interface LLMChainInput<\n  T extends string | object = string,\n  Model extends LLMType = LLMType,\n> extends ChainInputs {\n  /** Prompt object to use */\n  prompt: BasePromptTemplate;\n  /** LLM Wrapper to use */\n  llm: Model;\n  /** Kwargs to pass to LLM */\n  llmKwargs?: CallOptionsIfAvailable<Model>;\n  /** OutputParser to use */\n  outputParser?: BaseLLMOutputParser<T>;\n  /** Key to use for output, defaults to `text` */\n  outputKey?: string;\n}\n\nfunction isBaseLanguageModel(llmLike: unknown): llmLike is BaseLanguageModel {\n  return typeof (llmLike as BaseLanguageModelInterface)._llmType === \"function\";\n}\n\nfunction _getLanguageModel(llmLike: RunnableInterface): BaseLanguageModel {\n  if (isBaseLanguageModel(llmLike)) {\n    return llmLike;\n  } else if (\"bound\" in llmLike && Runnable.isRunnable(llmLike.bound)) {\n    return _getLanguageModel(llmLike.bound);\n  } else if (\n    \"runnable\" in llmLike &&\n    \"fallbacks\" in llmLike &&\n    Runnable.isRunnable(llmLike.runnable)\n  ) {\n    return _getLanguageModel(llmLike.runnable);\n  } else if (\"default\" in llmLike && Runnable.isRunnable(llmLike.default)) {\n    return _getLanguageModel(llmLike.default);\n  } else {\n    throw new Error(\"Unable to extract BaseLanguageModel from llmLike object.\");\n  }\n}\n\n/**\n * Chain to run queries against LLMs.\n *\n * @example\n * ```ts\n * import { ChatPromptTemplate } from \"@langchain/core/prompts\";\n * import { ChatOpenAI } from \"@langchain/openai\";\n *\n * const prompt = ChatPromptTemplate.fromTemplate(\"Tell me a {adjective} joke\");\n * const llm = new ChatOpenAI({ model: \"gpt-4o-mini\" });\n * const chain = prompt.pipe(llm);\n *\n * const response = await chain.invoke({ adjective: \"funny\" });\n * ```\n */\nexport class LLMChain<\n  T extends string | object = string,\n  Model extends LLMType = LLMType,\n>\n  extends BaseChain\n  implements LLMChainInput<T>\n{\n  static lc_name() {\n    return \"LLMChain\";\n  }\n\n  lc_serializable = true;\n\n  prompt: BasePromptTemplate;\n\n  llm: Model;\n\n  llmKwargs?: CallOptionsIfAvailable<Model>;\n\n  outputKey = \"text\";\n\n  outputParser?: BaseLLMOutputParser<T>;\n\n  get inputKeys() {\n    return this.prompt.inputVariables;\n  }\n\n  get outputKeys() {\n    return [this.outputKey];\n  }\n\n  constructor(fields: LLMChainInput<T, Model>) {\n    super(fields);\n    this.prompt = fields.prompt;\n    this.llm = fields.llm;\n    this.llmKwargs = fields.llmKwargs;\n    this.outputKey = fields.outputKey ?? this.outputKey;\n    this.outputParser =\n      fields.outputParser ?? (new NoOpOutputParser() as BaseOutputParser<T>);\n    if (this.prompt.outputParser) {\n      if (fields.outputParser) {\n        throw new Error(\"Cannot set both outputParser and prompt.outputParser\");\n      }\n      this.outputParser = this.prompt.outputParser as BaseOutputParser<T>;\n    }\n  }\n\n  private getCallKeys(): string[] {\n    const callKeys = \"callKeys\" in this.llm ? this.llm.callKeys : [];\n    return callKeys;\n  }\n\n  /** @ignore */\n  _selectMemoryInputs(values: ChainValues): ChainValues {\n    const valuesForMemory = super._selectMemoryInputs(values);\n    const callKeys = this.getCallKeys();\n    for (const key of callKeys) {\n      if (key in values) {\n        delete valuesForMemory[key];\n      }\n    }\n    return valuesForMemory;\n  }\n\n  /** @ignore */\n  async _getFinalOutput(\n    generations: Generation[],\n    promptValue: BasePromptValueInterface,\n    runManager?: CallbackManagerForChainRun\n  ): Promise<unknown> {\n    let finalCompletion: unknown;\n    if (this.outputParser) {\n      finalCompletion = await this.outputParser.parseResultWithPrompt(\n        generations,\n        promptValue,\n        runManager?.getChild()\n      );\n    } else {\n      finalCompletion = generations[0].text;\n    }\n    return finalCompletion;\n  }\n\n  /**\n   * Run the core logic of this chain and add to output if desired.\n   *\n   * Wraps _call and handles memory.\n   */\n  call(\n    values: ChainValues & CallOptionsIfAvailable<Model>,\n    config?: Callbacks | BaseCallbackConfig\n  ): Promise<ChainValues> {\n    return super.call(values, config);\n  }\n\n  /** @ignore */\n  async _call(\n    values: ChainValues & CallOptionsIfAvailable<Model>,\n    runManager?: CallbackManagerForChainRun\n  ): Promise<ChainValues> {\n    const valuesForPrompt = { ...values };\n    const valuesForLLM = {\n      ...this.llmKwargs,\n    } as CallOptionsIfAvailable<Model>;\n    const callKeys = this.getCallKeys();\n    for (const key of callKeys) {\n      if (key in values) {\n        if (valuesForLLM) {\n          valuesForLLM[key as keyof CallOptionsIfAvailable<Model>] =\n            values[key];\n          delete valuesForPrompt[key];\n        }\n      }\n    }\n    const promptValue = await this.prompt.formatPromptValue(valuesForPrompt);\n    if (\"generatePrompt\" in this.llm) {\n      const { generations } = await this.llm.generatePrompt(\n        [promptValue],\n        valuesForLLM,\n        runManager?.getChild()\n      );\n      return {\n        [this.outputKey]: await this._getFinalOutput(\n          generations[0],\n          promptValue,\n          runManager\n        ),\n      };\n    }\n\n    const modelWithParser = this.outputParser\n      ? this.llm.pipe(this.outputParser)\n      : this.llm;\n    const response = await modelWithParser.invoke(\n      promptValue,\n      runManager?.getChild()\n    );\n    return {\n      [this.outputKey]: response,\n    };\n  }\n\n  /**\n   * Format prompt with values and pass to LLM\n   *\n   * @param values - keys to pass to prompt template\n   * @param callbackManager - CallbackManager to use\n   * @returns Completion from LLM.\n   *\n   * @example\n   * ```ts\n   * llm.predict({ adjective: \"funny\" })\n   * ```\n   */\n  async predict(\n    values: ChainValues & CallOptionsIfAvailable<Model>,\n    callbackManager?: CallbackManager\n  ): Promise<T> {\n    const output = await this.call(values, callbackManager);\n    return output[this.outputKey];\n  }\n\n  _chainType() {\n    return \"llm\" as const;\n  }\n\n  static async deserialize(data: SerializedLLMChain): Promise<LLMChain> {\n    const { llm, prompt } = data;\n    if (!llm) {\n      throw new Error(\"LLMChain must have llm\");\n    }\n    if (!prompt) {\n      throw new Error(\"LLMChain must have prompt\");\n    }\n\n    return new LLMChain({\n      llm: await BaseLanguageModel.deserialize(llm),\n      prompt: await BasePromptTemplate.deserialize(prompt),\n    });\n  }\n\n  /** @deprecated */\n  serialize(): SerializedLLMChain {\n    const serialize =\n      \"serialize\" in this.llm ? this.llm.serialize() : undefined;\n    return {\n      _type: `${this._chainType()}_chain`,\n      llm: serialize,\n      prompt: this.prompt.serialize(),\n    };\n  }\n\n  _getNumTokens(text: string): Promise<number> {\n    return _getLanguageModel(this.llm).getNumTokens(text);\n  }\n}\n"],"mappings":";;;;;;;;AAmDA,SAAS,oBAAoBA,SAAgD;AAC3E,QAAO,OAAQ,QAAuC,aAAa;AACpE;AAED,SAAS,kBAAkBC,SAA+C;AACxE,KAAI,oBAAoB,QAAQ,CAC9B,QAAO;UACE,WAAW,WAAWC,oCAAS,WAAW,QAAQ,MAAM,CACjE,QAAO,kBAAkB,QAAQ,MAAM;UAEvC,cAAc,WACd,eAAe,WACfA,oCAAS,WAAW,QAAQ,SAAS,CAErC,QAAO,kBAAkB,QAAQ,SAAS;UACjC,aAAa,WAAWA,oCAAS,WAAW,QAAQ,QAAQ,CACrE,QAAO,kBAAkB,QAAQ,QAAQ;KAEzC,OAAM,IAAI,MAAM;AAEnB;;;;;;;;;;;;;;;;AAiBD,IAAa,WAAb,MAAa,iBAIHC,uBAEV;CACE,OAAO,UAAU;AACf,SAAO;CACR;CAED,kBAAkB;CAElB;CAEA;CAEA;CAEA,YAAY;CAEZ;CAEA,IAAI,YAAY;AACd,SAAO,KAAK,OAAO;CACpB;CAED,IAAI,aAAa;AACf,SAAO,CAAC,KAAK,SAAU;CACxB;CAED,YAAYC,QAAiC;EAC3C,MAAM,OAAO;EACb,KAAK,SAAS,OAAO;EACrB,KAAK,MAAM,OAAO;EAClB,KAAK,YAAY,OAAO;EACxB,KAAK,YAAY,OAAO,aAAa,KAAK;EAC1C,KAAK,eACH,OAAO,gBAAiB,IAAIC;AAC9B,MAAI,KAAK,OAAO,cAAc;AAC5B,OAAI,OAAO,aACT,OAAM,IAAI,MAAM;GAElB,KAAK,eAAe,KAAK,OAAO;EACjC;CACF;CAED,AAAQ,cAAwB;EAC9B,MAAM,WAAW,cAAc,KAAK,MAAM,KAAK,IAAI,WAAW,CAAE;AAChE,SAAO;CACR;;CAGD,oBAAoBC,QAAkC;EACpD,MAAM,kBAAkB,MAAM,oBAAoB,OAAO;EACzD,MAAM,WAAW,KAAK,aAAa;AACnC,OAAK,MAAM,OAAO,SAChB,KAAI,OAAO,QACT,OAAO,gBAAgB;AAG3B,SAAO;CACR;;CAGD,MAAM,gBACJC,aACAC,aACAC,YACkB;EAClB,IAAIC;AACJ,MAAI,KAAK,cACP,kBAAkB,MAAM,KAAK,aAAa,sBACxC,aACA,aACA,YAAY,UAAU,CACvB;OAED,kBAAkB,YAAY,GAAG;AAEnC,SAAO;CACR;;;;;;CAOD,KACEC,QACAC,QACsB;AACtB,SAAO,MAAM,KAAK,QAAQ,OAAO;CAClC;;CAGD,MAAM,MACJD,QACAF,YACsB;EACtB,MAAM,kBAAkB,EAAE,GAAG,OAAQ;EACrC,MAAM,eAAe,EACnB,GAAG,KAAK,UACT;EACD,MAAM,WAAW,KAAK,aAAa;AACnC,OAAK,MAAM,OAAO,SAChB,KAAI,OAAO,QACT;OAAI,cAAc;IAChB,aAAa,OACX,OAAO;IACT,OAAO,gBAAgB;GACxB;;EAGL,MAAM,cAAc,MAAM,KAAK,OAAO,kBAAkB,gBAAgB;AACxE,MAAI,oBAAoB,KAAK,KAAK;GAChC,MAAM,EAAE,aAAa,GAAG,MAAM,KAAK,IAAI,eACrC,CAAC,WAAY,GACb,cACA,YAAY,UAAU,CACvB;AACD,UAAO,GACJ,KAAK,YAAY,MAAM,KAAK,gBAC3B,YAAY,IACZ,aACA,WACD,CACF;EACF;EAED,MAAM,kBAAkB,KAAK,eACzB,KAAK,IAAI,KAAK,KAAK,aAAa,GAChC,KAAK;EACT,MAAM,WAAW,MAAM,gBAAgB,OACrC,aACA,YAAY,UAAU,CACvB;AACD,SAAO,GACJ,KAAK,YAAY,SACnB;CACF;;;;;;;;;;;;;CAcD,MAAM,QACJE,QACAE,iBACY;EACZ,MAAM,SAAS,MAAM,KAAK,KAAK,QAAQ,gBAAgB;AACvD,SAAO,OAAO,KAAK;CACpB;CAED,aAAa;AACX,SAAO;CACR;CAED,aAAa,YAAYC,MAA6C;EACpE,MAAM,EAAE,KAAK,QAAQ,GAAG;AACxB,MAAI,CAAC,IACH,OAAM,IAAI,MAAM;AAElB,MAAI,CAAC,OACH,OAAM,IAAI,MAAM;AAGlB,SAAO,IAAI,SAAS;GAClB,KAAK,MAAMC,wDAAkB,YAAY,IAAI;GAC7C,QAAQ,MAAMC,4CAAmB,YAAY,OAAO;EACrD;CACF;;CAGD,YAAgC;EAC9B,MAAM,YACJ,eAAe,KAAK,MAAM,KAAK,IAAI,WAAW,GAAG;AACnD,SAAO;GACL,OAAO,GAAG,KAAK,YAAY,CAAC,MAAM,CAAC;GACnC,KAAK;GACL,QAAQ,KAAK,OAAO,WAAW;EAChC;CACF;CAED,cAAcC,MAA+B;AAC3C,SAAO,kBAAkB,KAAK,IAAI,CAAC,aAAa,KAAK;CACtD;AACF"}