{"version":3,"file":"config.d.cts","names":["BaseLanguageModel","RunnableConfig","Example","Run","EvaluationResult","RunEvaluator","Criteria","CriteriaType","EmbeddingDistanceEvalChainInput","LoadEvaluatorOptions","EvaluatorType","EvaluatorInputs","EvaluatorInputFormatter","rawInput","rawPrediction","rawReferenceOutput","run","DynamicRunEvaluatorParams","Input","Prediction","Reference","Record","RunEvaluatorLike","Promise","isOffTheShelfEvaluator","T","U","EvalConfig","isCustomEvaluator","RunEvalType","RunEvalConfig","CriteriaEvalChainConfig","LabeledCriteria","Partial","Pick","EmbeddingDistance"],"sources":["../../src/smith/config.d.ts"],"sourcesContent":["import { BaseLanguageModel } from \"@langchain/core/language_models/base\";\nimport { RunnableConfig } from \"@langchain/core/runnables\";\nimport { Example, Run } from \"langsmith\";\nimport { EvaluationResult, RunEvaluator } from \"langsmith/evaluation\";\nimport { Criteria as CriteriaType, type EmbeddingDistanceEvalChainInput } from \"../evaluation/index.js\";\nimport { LoadEvaluatorOptions } from \"../evaluation/loader.js\";\nimport { EvaluatorType } from \"../evaluation/types.js\";\nexport type EvaluatorInputs = {\n    input?: string | unknown;\n    prediction: string | unknown;\n    reference?: string | unknown;\n};\nexport type EvaluatorInputFormatter = ({ rawInput, rawPrediction, rawReferenceOutput, run }: {\n    rawInput: any;\n    rawPrediction: any;\n    rawReferenceOutput?: any;\n    run: Run;\n}) => EvaluatorInputs;\nexport type DynamicRunEvaluatorParams<Input extends Record<string, any> = Record<string, unknown>, Prediction extends Record<string, any> = Record<string, unknown>, Reference extends Record<string, any> = Record<string, unknown>> = {\n    input: Input;\n    prediction?: Prediction;\n    reference?: Reference;\n    run: Run;\n    example?: Example;\n};\n/**\n * Type of a function that can be coerced into a RunEvaluator function.\n * While we have the class-based RunEvaluator, it's often more convenient to directly\n * pass a function to the runner. This type allows us to do that.\n */\nexport type RunEvaluatorLike = ((props: DynamicRunEvaluatorParams, options: RunnableConfig) => Promise<EvaluationResult>) | ((props: DynamicRunEvaluatorParams, options: RunnableConfig) => EvaluationResult);\nexport declare function isOffTheShelfEvaluator<T extends keyof EvaluatorType, U extends RunEvaluator | RunEvaluatorLike = RunEvaluator | RunEvaluatorLike>(evaluator: T | EvalConfig | U): evaluator is T | EvalConfig;\nexport declare function isCustomEvaluator<T extends keyof EvaluatorType, U extends RunEvaluator | RunEvaluatorLike = RunEvaluator | RunEvaluatorLike>(evaluator: T | EvalConfig | U): evaluator is U;\nexport type RunEvalType<T extends keyof EvaluatorType = \"criteria\" | \"labeled_criteria\" | \"embedding_distance\", U extends RunEvaluator | RunEvaluatorLike = RunEvaluator | RunEvaluatorLike> = T | EvalConfig | U;\n/**\n * Configuration class for running evaluations on datasets.\n *\n * @remarks\n * RunEvalConfig in LangSmith is a configuration class for running evaluations on datasets. Its primary purpose is to define the parameters and evaluators that will be applied during the evaluation of a dataset. This configuration can include various evaluators, custom evaluators, and different keys for inputs, predictions, and references.\n *\n * @typeparam T - The type of evaluators.\n * @typeparam U - The type of custom evaluators.\n */\nexport type RunEvalConfig<T extends keyof EvaluatorType = \"criteria\" | \"labeled_criteria\" | \"embedding_distance\", U extends RunEvaluator | RunEvaluatorLike = RunEvaluator | RunEvaluatorLike> = {\n    /**\n     * Evaluators to apply to a dataset run.\n     * You can optionally specify these by name, or by\n     * configuring them with an EvalConfig object.\n     */\n    evaluators?: RunEvalType<T, U>[];\n    /**\n     * Convert the evaluation data into formats that can be used by the evaluator.\n     * This should most commonly be a string.\n     * Parameters are the raw input from the run, the raw output, raw reference output, and the raw run.\n     * @example\n     * ```ts\n     * // Chain input: { input: \"some string\" }\n     * // Chain output: { output: \"some output\" }\n     * // Reference example output format: { output: \"some reference output\" }\n     * const formatEvaluatorInputs = ({\n     *   rawInput,\n     *   rawPrediction,\n     *   rawReferenceOutput,\n     * }) => {\n     *   return {\n     *     input: rawInput.input,\n     *     prediction: rawPrediction.output,\n     *     reference: rawReferenceOutput.output,\n     *   };\n     * };\n     * ```\n     * @returns The prepared data.\n     */\n    formatEvaluatorInputs?: EvaluatorInputFormatter;\n    /**\n     * Custom evaluators to apply to a dataset run.\n     * Each evaluator is provided with a run trace containing the model\n     * outputs, as well as an \"example\" object representing a record\n     * in the dataset.\n     *\n     * @deprecated Use `evaluators` instead.\n     */\n    customEvaluators?: U[];\n};\nexport interface EvalConfig extends LoadEvaluatorOptions {\n    /**\n     * The name of the evaluator to use.\n     * Example: labeled_criteria, criteria, etc.\n     */\n    evaluatorType: keyof EvaluatorType;\n    /**\n     * The feedback (or metric) name to use for the logged\n     * evaluation results. If none provided, we default to\n     * the evaluationName.\n     */\n    feedbackKey?: string;\n    /**\n     * Convert the evaluation data into formats that can be used by the evaluator.\n     * This should most commonly be a string.\n     * Parameters are the raw input from the run, the raw output, raw reference output, and the raw run.\n     * @example\n     * ```ts\n     * // Chain input: { input: \"some string\" }\n     * // Chain output: { output: \"some output\" }\n     * // Reference example output format: { output: \"some reference output\" }\n     * const formatEvaluatorInputs = ({\n     *   rawInput,\n     *   rawPrediction,\n     *   rawReferenceOutput,\n     * }) => {\n     *   return {\n     *     input: rawInput.input,\n     *     prediction: rawPrediction.output,\n     *     reference: rawReferenceOutput.output,\n     *   };\n     * };\n     * ```\n     * @returns The prepared data.\n     */\n    formatEvaluatorInputs: EvaluatorInputFormatter;\n}\n/**\n * Configuration to load a \"CriteriaEvalChain\" evaluator,\n * which prompts an LLM to determine whether the model's\n * prediction complies with the provided criteria.\n * @param criteria - The criteria to use for the evaluator.\n * @param llm - The language model to use for the evaluator.\n * @returns The configuration for the evaluator.\n * @example\n * ```ts\n * const evalConfig = {\n *   evaluators: [Criteria(\"helpfulness\")],\n * };\n * @example\n * ```ts\n * const evalConfig = {\n *   evaluators: [\n *     Criteria({\n *       \"isCompliant\": \"Does the submission comply with the requirements of XYZ\"\n *     })\n *   ],\n * };\n * @example\n * ```ts\n * const evalConfig = {\n *   evaluators: [{\n *     evaluatorType: \"criteria\",\n *     criteria: \"helpfulness\"\n *     formatEvaluatorInputs: ...\n *   }]\n * };\n * ```\n * @example\n * ```ts\n * const evalConfig = {\n *   evaluators: [{\n *     evaluatorType: \"criteria\",\n *     criteria: { \"isCompliant\": \"Does the submission comply with the requirements of XYZ\" },\n *     formatEvaluatorInputs: ...\n *   }]\n * };\n */\nexport type Criteria = EvalConfig & {\n    evaluatorType: \"criteria\";\n    /**\n     * The \"criteria\" to insert into the prompt template\n     * used for evaluation. See the prompt at\n     * https://smith.langchain.com/hub/langchain-ai/criteria-evaluator\n     * for more information.\n     */\n    criteria?: CriteriaType | Record<string, string>;\n    /**\n     * The language model to use as the evaluator, defaults to GPT-4\n     */\n    llm?: BaseLanguageModel;\n};\nexport type CriteriaEvalChainConfig = Criteria;\nexport declare function Criteria(criteria: CriteriaType | Record<string, string>, config?: Pick<Partial<LabeledCriteria>, \"formatEvaluatorInputs\" | \"llm\" | \"feedbackKey\">): EvalConfig;\n/**\n * Configuration to load a \"LabeledCriteriaEvalChain\" evaluator,\n * which prompts an LLM to determine whether the model's\n * prediction complies with the provided criteria and also\n * provides a \"ground truth\" label for the evaluator to incorporate\n * in its evaluation.\n * @param criteria - The criteria to use for the evaluator.\n * @param llm - The language model to use for the evaluator.\n * @returns The configuration for the evaluator.\n * @example\n * ```ts\n * const evalConfig = {\n *   evaluators: [LabeledCriteria(\"correctness\")],\n * };\n * @example\n * ```ts\n * const evalConfig = {\n *   evaluators: [\n *     LabeledCriteria({\n *       \"mentionsAllFacts\": \"Does the include all facts provided in the reference?\"\n *     })\n *   ],\n * };\n * @example\n * ```ts\n * const evalConfig = {\n *   evaluators: [{\n *     evaluatorType: \"labeled_criteria\",\n *     criteria: \"correctness\",\n *     formatEvaluatorInputs: ...\n *   }],\n * };\n * ```\n * @example\n * ```ts\n * const evalConfig = {\n *   evaluators: [{\n *     evaluatorType: \"labeled_criteria\",\n *     criteria: { \"mentionsAllFacts\": \"Does the include all facts provided in the reference?\" },\n *     formatEvaluatorInputs: ...\n *   }],\n * };\n */\nexport type LabeledCriteria = EvalConfig & {\n    evaluatorType: \"labeled_criteria\";\n    /**\n     * The \"criteria\" to insert into the prompt template\n     * used for evaluation. See the prompt at\n     * https://smith.langchain.com/hub/langchain-ai/labeled-criteria\n     * for more information.\n     */\n    criteria?: CriteriaType | Record<string, string>;\n    /**\n     * The language model to use as the evaluator, defaults to GPT-4\n     */\n    llm?: BaseLanguageModel;\n};\nexport declare function LabeledCriteria(criteria: CriteriaType | Record<string, string>, config?: Pick<Partial<LabeledCriteria>, \"formatEvaluatorInputs\" | \"llm\" | \"feedbackKey\">): LabeledCriteria;\n/**\n * Configuration to load a \"EmbeddingDistanceEvalChain\" evaluator,\n * which embeds distances to score semantic difference between\n * a prediction and reference.\n */\nexport type EmbeddingDistance = EvalConfig & EmbeddingDistanceEvalChainInput & {\n    evaluatorType: \"embedding_distance\";\n};\nexport declare function EmbeddingDistance(distanceMetric: EmbeddingDistanceEvalChainInput[\"distanceMetric\"], config?: Pick<Partial<LabeledCriteria>, \"formatEvaluatorInputs\" | \"embedding\" | \"feedbackKey\">): EmbeddingDistance;\n//# sourceMappingURL=config.d.ts.map"],"mappings":";;;;;;;;;;KAOYW,eAAAA;;;EAAAA,SAAAA,CAAAA,EAAAA,MAAe,GAAA,OAAA;AAK3B,CAAA;AAAyCE,KAA7BD,uBAAAA,GAA6BC,CAAAA;EAAAA,QAAAA;EAAAA,aAAAA;EAAAA,kBAAAA;EAAAA;CAAAA,EAAAA;EAAUC,QAAAA,EAAAA,GAAAA;EAAeC,aAAAA,EAAAA,GAAAA;EAAoBC,kBAAAA,CAAAA,EAAAA,GAAAA;EAI7Eb,GAAAA,EAAAA,GAAAA;CACHQ,EAAAA,GAAAA,eAAAA;AAAe,KACTM,yBADS,CAAA,cAC+BI,MAD/B,CAAA,MAAA,EAAA,GAAA,CAAA,GACqDA,MADrD,CAAA,MAAA,EAAA,OAAA,CAAA,EAAA,mBACiGA,MADjG,CAAA,MAAA,EAAA,GAAA,CAAA,GACuHA,MADvH,CAAA,MAAA,EAAA,OAAA,CAAA,EAAA,kBACkKA,MADlK,CAAA,MAAA,EAAA,GAAA,CAAA,GACwLA,MADxL,CAAA,MAAA,EAAA,OAAA,CAAA,CAAA,GAAA;EACTJ,KAAAA,EACDC,KADCD;EAAwCI,UAAAA,CAAAA,EAEnCF,UAFmCE;EAAsBA,SAAAA,CAAAA,EAG1DD,SAH0DC;EAA4CA,GAAAA,EAI7GlB,GAJ6GkB;EAAsBA,OAAAA,CAAAA,EAK9HnB,OAL8HmB;CAA2CA;;;;;;AAKzKnB,KAOFoB,gBAAAA,GAPEpB,CAAAA,CAAAA,KAAAA,EAO0Be,yBAP1Bf,EAAAA,OAAAA,EAO8DD,cAP9DC,EAAAA,GAOiFqB,OAPjFrB,CAOyFE,gBAPzFF,CAAAA,CAAAA,GAAAA,CAAAA,CAAAA,KAAAA,EAOuHe,yBAPvHf,EAAAA,OAAAA,EAO2JD,cAP3JC,EAAAA,GAO8KE,gBAP9KF,CAAAA;AAAO,iBAQGsB,sBARH,CAAA,UAAA,MAQ0Cd,aAR1C,EAAA,UAQmEL,YARnE,GAQkFiB,gBARlF,GAQqGjB,YARrG,GAQoHiB,gBARpH,CAAA,CAAA,SAAA,EAQiJG,CARjJ,GAQqJE,UARrJ,GAQkKD,CARlK,CAAA,EAAA,SAAA,IAQmLD,CARnL,GAQuLE,UARvL;AAOTL,iBAEYM,iBAFI,CAAA,UAAA,MAE8BlB,aAF9B,EAAA,UAEuDL,YAFvD,GAEsEiB,gBAFtE,GAEyFjB,YAFzF,GAEwGiB,gBAFxG,CAAA,CAAA,SAAA,EAEqIG,CAFrI,GAEyIE,UAFzI,GAEsJD,CAFtJ,CAAA,EAAA,SAAA,IAEuKA,CAFvK;AAAYT,KAG5BY,WAH4BZ,CAAAA,UAAAA,MAGAP,aAHAO,GAAAA,UAAAA,GAAAA,kBAAAA,GAAAA,oBAAAA,EAAAA,UAGkFZ,YAHlFY,GAGiGK,gBAHjGL,GAGoHZ,YAHpHY,GAGmIK,gBAHnIL,CAAAA,GAGuJQ,CAHvJR,GAG2JU,UAH3JV,GAGwKS,CAHxKT;;;;;;;AAAoK;AAC5M;;AAAwFZ,KAY5EyB,aAZ4EzB,CAAAA,UAAAA,MAY9CK,aAZ8CL,GAAAA,UAAAA,GAAAA,kBAAAA,GAAAA,oBAAAA,EAAAA,UAYoCA,YAZpCA,GAYmDiB,gBAZnDjB,GAYsEA,YAZtEA,GAYqFiB,gBAZrFjB,CAAAA,GAAAA;EAAeiB;;;;;EAAgFI,UAAAA,CAAAA,EAkBtKG,WAlBsKH,CAkB1JD,CAlB0JC,EAkBvJA,CAlBuJA,CAAAA,EAAAA;EAAiBD;;AAAc;AACtN;;;;;;;;;;AAAoM;AACpM;;;;;;;;;EAAiN,qBAAA,CAAA,EAwCrLb,uBAxCqL;EAUrMkB;;;;;;;;EAMKD,gBAAAA,CAAAA,EAiCMH,CAjCNG,EAAAA;CAwBWjB;AASLc,UAENC,UAAAA,SAAmBlB,oBAFbiB,CAAAA;EAAC;AAExB;;;EAAoCjB,aAAAA,EAAAA,MAKXC,aALWD;EAAoB;AA8ExD;;;;EAYUT,WAAAA,CAAAA,EAAAA,MAAAA;EAAiB;AAE3B;AACA;;;;;;;AAAuL;AA4CvL;;;;;AAY2B;AAE3B;;;;;;;EAAmM,qBAAA,EApHxKY,uBAoHwK;AAMnM;AAGA;;;;;;AAA+N;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;KAlFnNN,UAAAA,GAAWqB;;;;;;;;aAQRpB,WAAec;;;;QAIpBrB;;KAEE+B,uBAAAA,GAA0BzB;iBACdA,UAAAA,WAAmBC,WAAec,iCAAiCa,KAAKD,QAAQD,qEAAqEL;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;KA4CjKK,eAAAA,GAAkBL;;;;;;;;aAQfpB,WAAec;;;;QAIpBrB;;iBAEcgC,eAAAA,WAA0BzB,WAAec,iCAAiCa,KAAKD,QAAQD,qEAAqEA;;;;;;KAMxKG,iBAAAA,GAAoBR,aAAanB;;;iBAGrB2B,iBAAAA,iBAAkC3B,4DAA4D0B,KAAKD,QAAQD,2EAA2EG"}