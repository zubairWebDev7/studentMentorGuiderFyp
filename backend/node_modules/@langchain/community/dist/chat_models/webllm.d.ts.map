{"version":3,"file":"webllm.d.ts","names":["SimpleChatModel","BaseChatModelParams","BaseLanguageModelCallOptions","CallbackManagerForLLMRun","BaseMessage","ChatGenerationChunk","webllm","WebLLMInputs","AppConfig","ChatOptions","WebLLMCallOptions","ChatWebLLM","MLCEngine","InitProgressCallback","Promise","AsyncGenerator"],"sources":["../../src/chat_models/webllm.d.ts"],"sourcesContent":["import { SimpleChatModel, type BaseChatModelParams } from \"@langchain/core/language_models/chat_models\";\nimport type { BaseLanguageModelCallOptions } from \"@langchain/core/language_models/base\";\nimport { CallbackManagerForLLMRun } from \"@langchain/core/callbacks/manager\";\nimport { BaseMessage } from \"@langchain/core/messages\";\nimport { ChatGenerationChunk } from \"@langchain/core/outputs\";\nimport * as webllm from \"@mlc-ai/web-llm\";\nexport interface WebLLMInputs extends BaseChatModelParams {\n    appConfig?: webllm.AppConfig;\n    chatOptions?: webllm.ChatOptions;\n    temperature?: number;\n    model: string;\n}\nexport interface WebLLMCallOptions extends BaseLanguageModelCallOptions {\n}\n/**\n * To use this model you need to have the `@mlc-ai/web-llm` module installed.\n * This can be installed using `npm install -S @mlc-ai/web-llm`.\n *\n * You can see a list of available model records here:\n * https://github.com/mlc-ai/web-llm/blob/main/src/config.ts\n * @example\n * ```typescript\n * // Initialize the ChatWebLLM model with the model record.\n * const model = new ChatWebLLM({\n *   model: \"Phi-3-mini-4k-instruct-q4f16_1-MLC\",\n *   chatOptions: {\n *     temperature: 0.5,\n *   },\n * });\n *\n * // Call the model with a message and await the response.\n * const response = await model.invoke([\n *   new HumanMessage({ content: \"My name is John.\" }),\n * ]);\n * ```\n */\nexport declare class ChatWebLLM extends SimpleChatModel<WebLLMCallOptions> {\n    static inputs: WebLLMInputs;\n    protected engine: webllm.MLCEngine;\n    appConfig?: webllm.AppConfig;\n    chatOptions?: webllm.ChatOptions;\n    temperature?: number;\n    model: string;\n    static lc_name(): string;\n    constructor(inputs: WebLLMInputs);\n    _llmType(): string;\n    initialize(progressCallback?: webllm.InitProgressCallback): Promise<void>;\n    reload(modelId: string, newChatOpts?: webllm.ChatOptions): Promise<void>;\n    _streamResponseChunks(messages: BaseMessage[], options: this[\"ParsedCallOptions\"], runManager?: CallbackManagerForLLMRun): AsyncGenerator<ChatGenerationChunk>;\n    _call(messages: BaseMessage[], options: this[\"ParsedCallOptions\"], runManager?: CallbackManagerForLLMRun): Promise<string>;\n}\n//# sourceMappingURL=webllm.d.ts.map"],"mappings":";;;;;;;;UAMiBO,YAAAA,SAAqBN;cACtBK,MAAAA,CAAOE;EADND,WAAAA,CAAAA,EAECD,MAAAA,CAAOG,WAFI;EACbH,WAAOE,CAAAA,EAAAA,MAAAA;EACLF,KAAOG,EAAAA,MAAAA;;AAFgC,UAMxCC,iBAAAA,SAA0BR,4BANc,CAAA,CAMzD;AAwBA;;;;;;;;;;;;;;;;;;;AAAuD;;;cAAlCS,UAAAA,SAAmBX,gBAAgBU;iBACrCH;oBACGD,MAAAA,CAAOM;cACbN,MAAAA,CAAOE;gBACLF,MAAAA,CAAOG;;;;sBAIDF;;gCAEUD,MAAAA,CAAOO,uBAAuBC;wCACtBR,MAAAA,CAAOG,cAAcK;kCAC3BV,gEAAgED,2BAA2BY,eAAeV;kBAC1HD,gEAAgED,2BAA2BW"}