{"version":3,"file":"alibaba_tongyi.d.ts","names":["CallbackManagerForLLMRun","BaseChatModel","BaseChatModelParams","BaseMessage","ChatResult","ChatGenerationChunk","TongyiMessageRole","TongyiMessage","ChatCompletionRequest","NonNullable","AlibabaTongyiChatInput","ChatAlibabaTongyi","Partial","Pick","Promise","AbortSignal","MessageEvent","AsyncGenerator"],"sources":["../../src/chat_models/alibaba_tongyi.d.ts"],"sourcesContent":["import { type CallbackManagerForLLMRun } from \"@langchain/core/callbacks/manager\";\nimport { BaseChatModel, type BaseChatModelParams } from \"@langchain/core/language_models/chat_models\";\nimport { type BaseMessage } from \"@langchain/core/messages\";\nimport { type ChatResult } from \"@langchain/core/outputs\";\nimport { ChatGenerationChunk } from \"@langchain/core/outputs\";\n/**\n * Type representing the role of a message in the Tongyi chat model.\n */\nexport type TongyiMessageRole = \"system\" | \"assistant\" | \"user\";\n/**\n * Interface representing a message in the Tongyi chat model.\n */\ninterface TongyiMessage {\n    role: TongyiMessageRole;\n    content: string;\n}\n/**\n * Interface representing a request for a chat completion.\n *\n * See https://help.aliyun.com/zh/dashscope/developer-reference/model-square/\n */\ninterface ChatCompletionRequest {\n    model: (string & NonNullable<unknown>) | \"qwen-turbo\" | \"qwen-plus\" | \"qwen-max\" | \"qwen-max-1201\" | \"qwen-max-longcontext\" | \"qwen-7b-chat\" | \"qwen-14b-chat\" | \"qwen-72b-chat\" | \"llama2-7b-chat-v2\" | \"llama2-13b-chat-v2\" | \"baichuan-7b-v1\" | \"baichuan2-13b-chat-v1\" | \"baichuan2-7b-chat-v1\" | \"chatglm3-6b\" | \"chatglm-6b-v2\";\n    input: {\n        messages: TongyiMessage[];\n    };\n    parameters: {\n        stream?: boolean;\n        result_format?: \"text\" | \"message\";\n        seed?: number | null;\n        max_tokens?: number | null;\n        top_p?: number | null;\n        top_k?: number | null;\n        repetition_penalty?: number | null;\n        temperature?: number | null;\n        enable_search?: boolean | null;\n        incremental_output?: boolean | null;\n    };\n}\n/**\n * Interface defining the input to the ChatAlibabaTongyi class.\n */\ninterface AlibabaTongyiChatInput {\n    /**\n     * Model name to use. Available options are: qwen-turbo, qwen-plus, qwen-max, or Other compatible models.\n     * Alias for `model`\n     * @default \"qwen-turbo\"\n     */\n    modelName: string;\n    /** Model name to use. Available options are: qwen-turbo, qwen-plus, qwen-max, or Other compatible models.\n     * @default \"qwen-turbo\"\n     */\n    model: string;\n    /** Whether to stream the results or not. Defaults to false. */\n    streaming?: boolean;\n    /** Messages to pass as a prefix to the prompt */\n    prefixMessages?: TongyiMessage[];\n    /**\n     * API key to use when making requests. Defaults to the value of\n     * `ALIBABA_API_KEY` environment variable.\n     */\n    alibabaApiKey?: string;\n    /**\n     * Region for the Alibaba Tongyi API endpoint.\n     *\n     * Available regions:\n     * - 'china' (default): https://dashscope.aliyuncs.com/compatible-mode/v1\n     * - 'singapore': https://dashscope-intl.aliyuncs.com/compatible-mode/v1\n     * - 'us': https://dashscope-us.aliyuncs.com/compatible-mode/v1\n     *\n     * @default \"china\"\n     */\n    region?: \"china\" | \"singapore\" | \"us\";\n    /** Amount of randomness injected into the response. Ranges\n     * from 0 to 1 (0 is not included). Use temp closer to 0 for analytical /\n     * multiple choice, and temp closer to 1 for creative\n     * and generative tasks. Defaults to 0.95.\n     */\n    temperature?: number;\n    /** Total probability mass of tokens to consider at each step. Range\n     * from 0 to 1.0. Defaults to 0.8.\n     */\n    topP?: number;\n    topK?: number;\n    enableSearch?: boolean;\n    maxTokens?: number;\n    seed?: number;\n    /** Penalizes repeated tokens according to frequency. Range\n     * from 1.0 to 2.0. Defaults to 1.0.\n     */\n    repetitionPenalty?: number;\n}\n/**\n * Wrapper around Ali Tongyi large language models that use the Chat endpoint.\n *\n * To use you should have the `ALIBABA_API_KEY`\n * environment variable set.\n *\n * @augments BaseLLM\n * @augments AlibabaTongyiInput\n * @example\n * ```typescript\n * // Default - uses China region\n * const qwen = new ChatAlibabaTongyi({\n *   alibabaApiKey: \"YOUR-API-KEY\",\n * });\n *\n * // Specify region explicitly\n * const qwen = new ChatAlibabaTongyi({\n *   model: \"qwen-turbo\",\n *   temperature: 1,\n *   region: \"singapore\", // or \"us\" or \"china\"\n *   alibabaApiKey: \"YOUR-API-KEY\",\n * });\n *\n * const messages = [new HumanMessage(\"Hello\")];\n *\n * await qwen.call(messages);\n * ```\n */\nexport declare class ChatAlibabaTongyi extends BaseChatModel implements AlibabaTongyiChatInput {\n    static lc_name(): string;\n    get callKeys(): string[];\n    get lc_secrets(): {\n        alibabaApiKey: string;\n    };\n    get lc_aliases(): undefined;\n    lc_serializable: boolean;\n    alibabaApiKey?: string;\n    streaming: boolean;\n    prefixMessages?: TongyiMessage[];\n    modelName: ChatCompletionRequest[\"model\"];\n    model: ChatCompletionRequest[\"model\"];\n    apiUrl: string;\n    maxTokens?: number | undefined;\n    temperature?: number | undefined;\n    topP?: number | undefined;\n    topK?: number | undefined;\n    repetitionPenalty?: number | undefined;\n    seed?: number | undefined;\n    enableSearch?: boolean | undefined;\n    region: \"china\" | \"singapore\" | \"us\";\n    /**\n     * Get the API URL based on the specified region.\n     *\n     * @param region - The region to get the URL for ('china', 'singapore', or 'us')\n     * @returns The base URL for the specified region\n     */\n    private getRegionBaseUrl;\n    constructor(fields?: Partial<AlibabaTongyiChatInput> & BaseChatModelParams);\n    /**\n     * Get the parameters used to invoke the model\n     */\n    invocationParams(): ChatCompletionRequest[\"parameters\"];\n    /**\n     * Get the identifying parameters for the model\n     */\n    identifyingParams(): ChatCompletionRequest[\"parameters\"] & Pick<ChatCompletionRequest, \"model\">;\n    /** @ignore */\n    _generate(messages: BaseMessage[], options?: this[\"ParsedCallOptions\"], runManager?: CallbackManagerForLLMRun): Promise<ChatResult>;\n    /** @ignore */\n    completionWithRetry(request: ChatCompletionRequest, stream: boolean, signal?: AbortSignal, onmessage?: (event: MessageEvent) => void): Promise<any>;\n    _streamResponseChunks(messages: BaseMessage[], options?: this[\"ParsedCallOptions\"], runManager?: CallbackManagerForLLMRun): AsyncGenerator<ChatGenerationChunk>;\n    private createTongyiStream;\n    _llmType(): string;\n    /** @ignore */\n    _combineLLMOutput(): never[];\n}\nexport {};\n//# sourceMappingURL=alibaba_tongyi.d.ts.map"],"mappings":";;;;;;;;AAQA;AAAgE;AAatDQ,KAbEF,iBAAAA,GAamB,QAAA,GACVG,WAEHF,GAAAA,MAAAA;AAAa;AAgG/B;;UA5GUA,aAAAA,CAuHKC;EACJA,IAAAA,EAvHDF,iBAuHCE;EAiBsBE,OAAAA,EAAAA,MAAAA;;;;;;;UAhIvBF,qBAAAA,CA0IcL;EAAiEH,KAAAA,EAAAA,CAAAA,MAAAA,GAzIpES,WAyIoET,CAAAA,OAAAA,CAAAA,CAAAA,GAAAA,YAAAA,GAAAA,WAAAA,GAAAA,UAAAA,GAAAA,eAAAA,GAAAA,sBAAAA,GAAAA,cAAAA,GAAAA,eAAAA,GAAAA,eAAAA,GAAAA,mBAAAA,GAAAA,oBAAAA,GAAAA,gBAAAA,GAAAA,uBAAAA,GAAAA,sBAAAA,GAAAA,aAAAA,GAAAA,eAAAA;EAAmCI,KAAAA,EAAAA;IAARU,QAAAA,EAvIlGP,aAuIkGO,EAAAA;EAEnFN,CAAAA;EAAiDO,UAAAA,EAAAA;IAAiCC,MAAAA,CAAAA,EAAAA,OAAAA;IAAwBF,aAAAA,CAAAA,EAAAA,MAAAA,GAAAA,SAAAA;IACvGX,IAAAA,CAAAA,EAAAA,MAAAA,GAAAA,IAAAA;IAAiEH,UAAAA,CAAAA,EAAAA,MAAAA,GAAAA,IAAAA;IAA0CK,KAAAA,CAAAA,EAAAA,MAAAA,GAAAA,IAAAA;IAAfY,KAAAA,CAAAA,EAAAA,MAAAA,GAAAA,IAAAA;IA1CjFhB,kBAAAA,CAAAA,EAAAA,MAAAA,GAAAA,IAAAA;IAAyBS,WAAAA,CAAAA,EAAAA,MAAAA,GAAAA,IAAAA;IAAsB,aAAA,CAAA,EAAA,OAAA,GAAA,IAAA;;;;;;;UA9EpFA,sBAAAA;;;;;;;;;;;;;;mBAcWH;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;cAgEAI,iBAAAA,SAA0BV,aAAAA,YAAyBS;;;;;;;;;;mBAUnDH;aACNC;SACJA;;;;;;;;;;;;;;;;;uBAiBcI,QAAQF,0BAA0BR;;;;sBAInCM;;;;uBAICA,sCAAsCK,KAAKL;;sBAE5CL,iEAAiEH,2BAA2Bc,QAAQV;;+BAE3FI,iDAAiDO,iCAAiCC,wBAAwBF;kCACvGX,iEAAiEH,2BAA2BiB,eAAeZ"}