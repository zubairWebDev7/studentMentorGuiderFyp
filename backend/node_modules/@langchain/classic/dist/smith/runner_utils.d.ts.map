{"version":3,"file":"runner_utils.d.ts","names":["Runnable","Client","Feedback","TraceableFunction","RunEvalConfig","ChainOrFactory","AnyTraceableFunction","Promise","RunOnDatasetParams","Record","Omit","EvalResults","runOnDataset"],"sources":["../../src/smith/runner_utils.d.ts"],"sourcesContent":["import { Runnable } from \"@langchain/core/runnables\";\nimport { Client, Feedback } from \"langsmith\";\nimport type { TraceableFunction } from \"langsmith/singletons/traceable\";\nimport { type RunEvalConfig } from \"./config.js\";\nexport type ChainOrFactory = Runnable | (() => Runnable) | AnyTraceableFunction | ((obj: any) => any) | ((obj: any) => Promise<any>) | (() => (obj: unknown) => unknown) | (() => (obj: unknown) => Promise<unknown>);\ntype AnyTraceableFunction = TraceableFunction<(...any: any[]) => any>;\nexport interface RunOnDatasetParams extends Omit<RunEvalConfig, \"customEvaluators\"> {\n    /**\n     * Name of the project for logging and tracking.\n     */\n    projectName?: string;\n    /**\n     * Additional metadata for the project.\n     */\n    projectMetadata?: Record<string, unknown>;\n    /**\n     * Client instance for LangSmith service interaction.\n     */\n    client?: Client;\n    /**\n     * Maximum concurrency level for dataset processing.\n     */\n    maxConcurrency?: number;\n    /**\n     * @deprecated Pass keys directly to the RunOnDatasetParams instead\n     */\n    evaluationConfig?: RunEvalConfig;\n}\nexport type EvalResults = {\n    projectName: string;\n    results: {\n        [key: string]: {\n            execution_time?: number;\n            run_id: string;\n            feedback: Feedback[];\n        };\n    };\n};\n/**\n * Evaluates a given model or chain against a specified LangSmith dataset.\n *\n * This function fetches example records from the specified dataset,\n * runs the model or chain against each example, and returns the evaluation\n * results.\n *\n * @param chainOrFactory - A model or factory/constructor function to be evaluated. It can be a\n * Runnable instance, a factory function that returns a Runnable, or a user-defined\n * function or factory.\n *\n * @param datasetName - The name of the dataset against which the evaluation will be\n * performed. This dataset should already be defined and contain the relevant data\n * for evaluation.\n *\n * @param options - (Optional) Additional parameters for the evaluation process:\n *   - `evaluators` (RunEvalType[]): Evaluators to apply to a dataset run.\n *   - `formatEvaluatorInputs` (EvaluatorInputFormatter): Convert the evaluation data into formats that can be used by the evaluator.\n *   - `projectName` (string): Name of the project for logging and tracking.\n *   - `projectMetadata` (Record<string, unknown>): Additional metadata for the project.\n *   - `client` (Client): Client instance for LangSmith service interaction.\n *   - `maxConcurrency` (number): Maximum concurrency level for dataset processing.\n *\n * @returns A promise that resolves to an `EvalResults` object. This object includes\n * detailed results of the evaluation, such as execution time, run IDs, and feedback\n * for each entry in the dataset.\n *\n * @example\n * ```typescript\n * // Example usage for evaluating a model on a dataset\n * async function evaluateModel() {\n *   const chain = /* ...create your model or chain...*\\//\n *   const datasetName = 'example-dataset';\n *   const client = new Client(/* ...config... *\\//);\n *\n *   const results = await runOnDataset(chain, datasetName, {\n *     evaluators: [/* ...evaluators... *\\//],\n *     client,\n *   });\n *\n *   console.log('Evaluation Results:', results);\n * }\n *\n * evaluateModel();\n * ```\n * In this example, `runOnDataset` is used to evaluate a language model (or a chain of models) against\n * a dataset named 'example-dataset'. The evaluation process is configured using `RunOnDatasetParams[\"evaluators\"]`, which can\n * include both standard and custom evaluators. The `Client` instance is used to interact with LangChain services.\n * The function returns the evaluation results, which can be logged or further processed as needed.\n */\nexport declare function runOnDataset(chainOrFactory: ChainOrFactory, datasetName: string, options?: RunOnDatasetParams): Promise<EvalResults>;\nexport {};\n//# sourceMappingURL=runner_utils.d.ts.map"],"mappings":";;;;;;KAIYK,cAAAA,GAAiBL,kBAAkBA,YAAYM,4DAA4DC,6EAA6EA;KAC/LD,oBAAAA,GAAuBH;AADhBE,UAEKG,kBAAAA,SAA2BE,IAFlB,CAEuBN,aAFvB,EAAA,kBAAA,CAAA,CAAA;EAAGJ;;;EAA0FO,WAAAA,CAAAA,EAAAA,MAAAA;EAA6EA;AAAO;AAAW;EAErMC,eAAAA,CAAAA,EAQKC,MARa,CAAA,MAAA,EAAA,OAAA,CAAA;EAAcL;;;EAoB1BA,MAAAA,CAAAA,EARVH,MAQUG;EApBqBM;AAAI;AAsBhD;EA4DwBE,cAAAA,CAAY,EAAA,MAAA;EAAiBP;;;EAAoEE,gBAAAA,CAAAA,EA9DlGH,aA8DkGG;AAAO;KA5DpHI,WAAAA;;;;;;gBAMUT;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;iBAsDEU,YAAAA,iBAA6BP,+CAA+CG,qBAAqBD,QAAQI"}