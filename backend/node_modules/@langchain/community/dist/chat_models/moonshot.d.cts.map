{"version":3,"file":"moonshot.d.cts","names":["BaseChatModel","BaseChatModelParams","BaseMessage","ChatResult","CallbackManagerForLLMRun","MoonshotMessageRole","MoonshotMessage","ModelName","NonNullable","ChatCompletionRequest","ChatMoonshotParams","ChatMoonshot","Partial","Omit","Promise","AbortSignal","MessageEvent"],"sources":["../../src/chat_models/moonshot.d.ts"],"sourcesContent":["import { BaseChatModel, type BaseChatModelParams } from \"@langchain/core/language_models/chat_models\";\nimport { type BaseMessage } from \"@langchain/core/messages\";\nimport { type ChatResult } from \"@langchain/core/outputs\";\nimport { type CallbackManagerForLLMRun } from \"@langchain/core/callbacks/manager\";\nexport type MoonshotMessageRole = \"system\" | \"assistant\" | \"user\";\ninterface MoonshotMessage {\n    role: MoonshotMessageRole;\n    content: string;\n}\n/**\n * Interface representing a request for a chat completion.\n *\n * See https://platform.moonshot.cn/docs/intro#%E6%A8%A1%E5%9E%8B%E5%88%97%E8%A1%A8\n */\ntype ModelName = (string & NonNullable<unknown>) | \"moonshot-v1-8k\" | \"moonshot-v1-32k\" | \"moonshot-v1-128k\";\ninterface ChatCompletionRequest {\n    model: ModelName;\n    messages?: MoonshotMessage[];\n    stream?: boolean;\n    max_tokens?: number | null;\n    top_p?: number | null;\n    temperature?: number | null;\n    stop?: string[];\n    presence_penalty?: number;\n    frequency_penalty?: number;\n    n?: number;\n}\n/**\n * Interface defining the input to the MoonshotChatInput class.\n */\nexport interface ChatMoonshotParams {\n    /**\n     * @default \"moonshot-v1-8k\"\n     * Alias for `model`\n     */\n    modelName: ModelName;\n    /**\n     * @default \"moonshot-v1-8k\"\n     */\n    model: ModelName;\n    /** Whether to stream the results or not. Defaults to false. */\n    streaming?: boolean;\n    /** Messages to pass as a prefix to the prompt */\n    messages?: MoonshotMessage[];\n    /**\n     * API key to use when making requests. Defaults to the value of\n     * `MOONSHOT_API_KEY` environment variable.\n     */\n    apiKey?: string;\n    /**\n     * Amount of randomness injected into the response. Ranges\n     * from 0 to 1 (0 is not included). Use temp closer to 0 for analytical /\n     * multiple choice, and temp closer to 1 for creative and generative tasks.\n     * Defaults to 0, recommended 0.3\n     */\n    temperature?: number;\n    /**\n     * Total probability mass of tokens to consider at each step. Range\n     * from 0 to 1. Defaults to 1\n     */\n    topP?: number;\n    /**\n     * Different models have different maximum values. For example, the maximum\n     * value of moonshot-v1-8k is 8192. Defaults to 1024\n     */\n    maxTokens?: number;\n    stop?: string[];\n    /**\n     * There is a penalty, a number between -2.0 and 2.0. Positive values\n     * penalize the newly generated words based on whether they appear in the\n     * text, increasing the likelihood that the model will discuss new topics.\n     * The default value is 0\n     */\n    presencePenalty?: number;\n    /**\n     * Frequency penalty, a number between -2.0 and 2.0. Positive values\n     * penalize the newly generated words based on their existing frequency in the\n     * text, making the model less likely to repeat the same words verbatim.\n     * The default value is 0\n     */\n    frequencyPenalty?: number;\n    /**\n     * The default value is 1 and cannot be greater than 5. In particular,\n     * when temperature is very small and close to 0, we can only return 1 result.\n     * If n is already set and > 1, Moonshot will return an invalid input parameter\n     * (invalid_request_error).\n     */\n    n?: number;\n}\nexport declare class ChatMoonshot extends BaseChatModel implements ChatMoonshotParams {\n    static lc_name(): string;\n    get callKeys(): string[];\n    get lc_secrets(): {\n        apiKey: string;\n    };\n    get lc_aliases(): undefined;\n    apiKey?: string;\n    streaming: boolean;\n    messages?: MoonshotMessage[];\n    modelName: ChatCompletionRequest[\"model\"];\n    model: ChatCompletionRequest[\"model\"];\n    apiUrl: string;\n    maxTokens?: number | undefined;\n    temperature?: number | undefined;\n    topP?: number | undefined;\n    stop?: string[];\n    presencePenalty?: number;\n    frequencyPenalty?: number;\n    n?: number;\n    constructor(fields?: Partial<ChatMoonshotParams> & BaseChatModelParams);\n    /**\n     * Get the parameters used to invoke the model\n     */\n    invocationParams(): Omit<ChatCompletionRequest, \"messages\">;\n    /**\n     * Get the identifying parameters for the model\n     */\n    identifyingParams(): Omit<ChatCompletionRequest, \"messages\">;\n    /** @ignore */\n    _generate(messages: BaseMessage[], options?: this[\"ParsedCallOptions\"], runManager?: CallbackManagerForLLMRun): Promise<ChatResult>;\n    /** @ignore */\n    completionWithRetry(request: ChatCompletionRequest, stream: boolean, signal?: AbortSignal, onmessage?: (event: MessageEvent) => void): Promise<any>;\n    _llmType(): string;\n    /** @ignore */\n    _combineLLMOutput(): never[];\n}\nexport {};\n//# sourceMappingURL=moonshot.d.ts.map"],"mappings":";;;;;;KAIYK,mBAAAA;UACFC,eAAAA;EADED,IAAAA,EAEFA,mBAFqB;EACrBC,OAAAA,EAAAA,MAAAA;AACmB;AAQS;AAgBtC;;;;AAa8B,KA7BzBC,SAAAA,GA6ByB,CAAA,MAAA,GA7BHC,WA6BG,CAAA,OAAA,CAAA,CAAA,GAAA,gBAAA,GAAA,iBAAA,GAAA,kBAAA;AA8C9B,UA1EUC,qBAAAA,CA0EuB;EASlBH,KAAAA,EAlFJC,SAkFID;EACAG,QAAAA,CAAAA,EAlFAH,eAkFAG,EAAAA;EACJA,MAAAA,CAAAA,EAAAA,OAAAA;EASsBC,UAAAA,CAAAA,EAAAA,MAAAA,GAAAA,IAAAA;EAARE,KAAAA,CAAAA,EAAAA,MAAAA,GAAAA,IAAAA;EAA8BX,WAAAA,CAAAA,EAAAA,MAAAA,GAAAA,IAAAA;EAI1BQ,IAAAA,CAAAA,EAAAA,MAAAA,EAAAA;EAALI,gBAAAA,CAAAA,EAAAA,MAAAA;EAIMJ,iBAAAA,CAAAA,EAAAA,MAAAA;EAALI,CAAAA,CAAAA,EAAAA,MAAAA;;;;;AAIQJ,UA3FhBC,kBAAAA,CA2FgBD;EAAiDM;;;;EAhCfL,SAAAA,EAtDpDH,SAsDoDG;EAAkB;;;SAlD1EH;;;;aAIID;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;cA8CMK,YAAAA,SAAqBX,aAAAA,YAAyBU;;;;;;;;;aASpDJ;aACAG;SACJA;;;;;;;;;uBAScG,QAAQF,sBAAsBT;;;;sBAI/BY,KAAKJ;;;;uBAIJI,KAAKJ;;sBAENP,iEAAiEE,2BAA2BU,QAAQX;;+BAE3FM,iDAAiDM,iCAAiCC,wBAAwBF"}