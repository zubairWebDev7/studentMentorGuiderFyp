{"version":3,"file":"token_buffer_memory.d.cts","names":["ChatOpenAI","InputValues","MemoryVariables","OutputValues","BaseChatMemory","BaseChatMemoryInput","OpenAIAgentTokenBufferMemoryFields","OpenAIAgentTokenBufferMemory","_langchain_core_messages2","MessageToolSet","MessageStructure","MessageType","BaseMessage","Promise"],"sources":["../../../../src/agents/toolkits/conversational_retrieval/token_buffer_memory.d.ts"],"sourcesContent":["import { ChatOpenAI } from \"@langchain/openai\";\nimport { InputValues, MemoryVariables, OutputValues } from \"@langchain/core/memory\";\nimport { BaseChatMemory, BaseChatMemoryInput } from \"../../../memory/chat_memory.js\";\n/**\n * Type definition for the fields required to initialize an instance of\n * OpenAIAgentTokenBufferMemory.\n */\nexport type OpenAIAgentTokenBufferMemoryFields = BaseChatMemoryInput & {\n    llm: ChatOpenAI;\n    humanPrefix?: string;\n    aiPrefix?: string;\n    memoryKey?: string;\n    maxTokenLimit?: number;\n    returnMessages?: boolean;\n    outputKey?: string;\n    intermediateStepsKey?: string;\n};\n/**\n * Memory used to save agent output and intermediate steps.\n */\nexport declare class OpenAIAgentTokenBufferMemory extends BaseChatMemory {\n    humanPrefix: string;\n    aiPrefix: string;\n    llm: ChatOpenAI;\n    memoryKey: string;\n    maxTokenLimit: number;\n    returnMessages: boolean;\n    outputKey: string;\n    intermediateStepsKey: string;\n    constructor(fields: OpenAIAgentTokenBufferMemoryFields);\n    get memoryKeys(): string[];\n    /**\n     * Retrieves the messages from the chat history.\n     * @returns Promise that resolves with the messages from the chat history.\n     */\n    getMessages(): Promise<import(\"@langchain/core/messages\").BaseMessage<import(\"@langchain/core/messages\").MessageStructure<import(\"@langchain/core/messages\").MessageToolSet>, import(\"@langchain/core/messages\").MessageType>[]>;\n    /**\n     * Loads memory variables from the input values.\n     * @param _values Input values.\n     * @returns Promise that resolves with the loaded memory variables.\n     */\n    loadMemoryVariables(_values: InputValues): Promise<MemoryVariables>;\n    /**\n     * Saves the context of the chat, including user input, AI output, and\n     * intermediate steps. Prunes the chat history if the total token count\n     * exceeds the maximum limit.\n     * @param inputValues Input values.\n     * @param outputValues Output values.\n     * @returns Promise that resolves when the context has been saved.\n     */\n    saveContext(inputValues: InputValues, outputValues: OutputValues): Promise<void>;\n}\n//# sourceMappingURL=token_buffer_memory.d.ts.map"],"mappings":";;;;;;;;;;AAOA;AAaqBO,KAbTD,kCAAAA,GAAqCD,mBAaA,GAAA;EAGxCL,GAAAA,EAfAA,UAeAA;EAMeM,WAAAA,CAAAA,EAAAA,MAAAA;EAAkCE,QAAAA,CAAAA,EAAAA,MAAAA;EAMqHA,SAAAA,CAAAA,EAAAA,MAAAA;EAAlDA,aAAAA,CAAAA,EAAAA,MAAAA;EAAmGA,cAAAA,CAAAA,EAAAA,OAAlKI;EAA3CC,SAAAA,CAAAA,EAAAA,MAAAA;EAMcZ,oBAAAA,CAAAA,EAAAA,MAAAA;CAAsBC;;;;AASgBW,cA9BlDN,4BAAAA,SAAqCH,cAAAA,CA8BaS;EA9BbT,WAAAA,EAAAA,MAAAA;EAAc,QAAA,EAAA,MAAA;OAG/DJ;;;;;;sBAMeM;;;;;;iBAMLO,QAA6ML,yBAAAA,CAAlKI,YAAiHJ,yBAAAA,CAAlEE,iBANnDF,yBAAAA,CAMuGC,cAAAA,GAApCD,yBAAAA,CAAwFG,WAAAA;;;;;;+BAMpLV,cAAcY,QAAQX;;;;;;;;;2BAS1BD,2BAA2BE,eAAeU"}