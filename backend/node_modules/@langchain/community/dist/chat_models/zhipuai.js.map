{"version":3,"file":"zhipuai.js","names":["message: BaseMessage","rawToolCalls: ToolCall[]","e: any","fields: Partial<ChatZhipuAIParams> & BaseChatModelParams","options?: this[\"ParsedCallOptions\"]","messages: BaseMessage[]","runManager?: CallbackManagerForLLMRun","messagesMapped: ZhipuMessage[]","response: ChatCompletionResponse","data: ChatCompletionResponse","data","text","tools: BindToolsInput[]","kwargs?: Partial<this[\"ParsedCallOptions\"]>","request: ChatCompletionRequest","stream: boolean","signal?: AbortSignal","onmessage?: (event: MessageEvent) => void","value","json: string"],"sources":["../../src/chat_models/zhipuai.ts"],"sourcesContent":["import {\n  BaseChatModel,\n  BaseChatModelCallOptions,\n  BindToolsInput,\n  type BaseChatModelParams,\n} from \"@langchain/core/language_models/chat_models\";\nimport {\n  AIMessage,\n  type BaseMessage,\n  ChatMessage,\n  AIMessageChunk,\n} from \"@langchain/core/messages\";\nimport { ChatGenerationChunk, type ChatResult } from \"@langchain/core/outputs\";\nimport { type CallbackManagerForLLMRun } from \"@langchain/core/callbacks/manager\";\nimport { getEnvironmentVariable } from \"@langchain/core/utils/env\";\n\nimport { encodeApiKey } from \"../utils/zhipuai.js\";\nimport { convertEventStreamToIterableReadableDataStream } from \"../utils/event_source_parse.js\";\nimport { Runnable } from \"@langchain/core/runnables\";\nimport {\n  BaseLanguageModelInput,\n  ToolDefinition,\n} from \"@langchain/core/language_models/base\";\nimport { convertToOpenAITool } from \"@langchain/core/utils/function_calling\";\nimport {\n  makeInvalidToolCall,\n  parseToolCall,\n} from \"@langchain/core/output_parsers/openai_tools\";\n\nexport type ZhipuMessageRole = \"system\" | \"assistant\" | \"user\";\n\ninterface ZhipuMessage {\n  role: ZhipuMessageRole;\n  content: string;\n}\n\n/**\n * Interface representing a request for a chat completion.\n *\n * See https://open.bigmodel.cn/dev/howuse/model\n */\ntype ModelName =\n  | (string & NonNullable<unknown>)\n  // will be deprecated models\n  | \"chatglm_pro\" // deprecated in 2024-12-31T23:59:59+0800，point to glm-4\n  | \"chatglm_std\" // deprecated in 2024-12-31T23:59:59+0800，point to glm-3-turbo\n  | \"chatglm_lite\" // deprecated in 2024-12-31T23:59:59+0800，point to glm-3-turbo\n  // GLM-4 more powerful on Q/A and text generation, suitable for complex dialog interactions and deep content creation design.\n  | \"glm-4\" // context size: 128k\n  | \"glm-4v\" // context size: 2k\n  // ChatGLM-Turbo\n  | \"glm-3-turbo\" // context size: 128k\n  | \"chatglm_turbo\"; // context size: 32k\n\nexport interface ChatZhipuAICallOptions extends BaseChatModelCallOptions {\n  tools?: BindToolsInput[];\n}\n\ninterface ChatCompletionRequest {\n  model: ModelName;\n  messages?: ZhipuMessage[];\n  tools?: ToolDefinition[];\n  do_sample?: boolean;\n  stream?: boolean;\n  request_id?: string;\n  max_tokens?: number | null;\n  top_p?: number | null;\n  top_k?: number | null;\n  temperature?: number | null;\n  stop?: string[];\n}\n\ninterface BaseResponse {\n  code?: string;\n  message?: string;\n}\n\ninterface ChoiceMessage {\n  role: string;\n  content?: string;\n  tool_calls?: ToolCall[];\n}\n\ninterface ToolCall {\n  id: string;\n  type: string;\n  index: number;\n  function: {\n    name: string;\n    arguments: string; // Model-generated function call parameters list in JSON format.\n  };\n}\n\ninterface ResponseChoice {\n  index: number;\n  finish_reason: \"stop\" | \"tool_calls\" | \"length\" | \"null\" | null;\n  delta: ChoiceMessage;\n  message: ChoiceMessage;\n}\n\ninterface ZhipuAIError {\n  error: BaseResponse;\n}\n\n/**\n * Interface representing a response from a chat completion.\n */\ninterface ChatCompletionResponse extends ZhipuAIError {\n  choices: ResponseChoice[];\n  created: number;\n  id: string;\n  model: string;\n  request_id: string;\n  usage: {\n    completion_tokens: number;\n    prompt_tokens: number;\n    total_tokens: number;\n  };\n  output: {\n    text: string;\n    finish_reason: \"stop\" | \"tool_calls\" | \"length\" | \"null\" | null;\n    tool_calls?: ToolCall[];\n  };\n}\n\n/**\n * Interface defining the input to the ZhipuAIChatInput class.\n */\nexport interface ChatZhipuAIParams {\n  /**\n   * @default \"glm-3-turbo\"\n   * Alias for `model`\n   */\n  modelName: ModelName;\n  /**\n   * @default \"glm-3-turbo\"\n   */\n  model: ModelName;\n\n  /** Whether to stream the results or not. Defaults to false. */\n  streaming?: boolean;\n\n  /** Messages to pass as a prefix to the prompt */\n  messages?: ZhipuMessage[];\n\n  /**\n   * API key to use when making requests. Defaults to the value of\n   * `ZHIPUAI_API_KEY` environment variable.\n   * Alias for `apiKey`\n   */\n  zhipuAIApiKey?: string;\n\n  /**\n   * API key to use when making requests. Defaults to the value of\n   * `ZHIPUAI_API_KEY` environment variable.\n   */\n  apiKey?: string;\n\n  /** Amount of randomness injected into the response. Ranges\n   * from 0 to 1 (0 is not included). Use temp closer to 0 for analytical /\n   * multiple choice, and temp closer to 1 for creative\n   * and generative tasks. Defaults to 0.95\n   */\n  temperature?: number;\n\n  /** Total probability mass of tokens to consider at each step. Range\n   * from 0 to 1 Defaults to 0.7\n   */\n  topP?: number;\n\n  /**\n   * Unique identifier for the request. Defaults to a random UUID.\n   */\n  requestId?: string;\n\n  /**\n   * turn on sampling strategy when do_sample is true,\n   * do_sample is false, temperature、top_p will not take effect\n   */\n  doSample?: boolean;\n\n  /**\n   * max value is 8192，defaults to 1024\n   */\n  maxTokens?: number;\n\n  stop?: string[];\n}\n\nfunction messageToRole(message: BaseMessage): ZhipuMessageRole {\n  const type = message._getType();\n  switch (type) {\n    case \"ai\":\n      return \"assistant\";\n    case \"human\":\n      return \"user\";\n    case \"system\":\n      return \"system\";\n    case \"function\":\n      throw new Error(\"Function messages not supported yet\");\n    case \"generic\": {\n      if (!ChatMessage.isInstance(message)) {\n        throw new Error(\"Invalid generic chat message\");\n      }\n      if ([\"system\", \"assistant\", \"user\"].includes(message.role)) {\n        return message.role as ZhipuMessageRole;\n      }\n      throw new Error(`Unknown message type: ${type}`);\n    }\n    default:\n      throw new Error(`Unknown message type: ${type}`);\n  }\n}\n\nfunction parseRawToolCalls(rawToolCalls: ToolCall[]) {\n  const toolCalls = [];\n  const invalidToolCalls = [];\n  for (const rawToolCall of rawToolCalls) {\n    try {\n      toolCalls.push(parseToolCall(rawToolCall, { returnId: true }));\n      // eslint-disable-next-line @typescript-eslint/no-explicit-any\n    } catch (e: any) {\n      invalidToolCalls.push(makeInvalidToolCall(rawToolCall, e.message));\n    }\n  }\n  return { toolCalls, invalidToolCalls };\n}\n\nexport class ChatZhipuAI\n  extends BaseChatModel<ChatZhipuAICallOptions>\n  implements ChatZhipuAIParams\n{\n  static lc_name() {\n    return \"ChatZhipuAI\";\n  }\n\n  get callKeys() {\n    return [\"stop\", \"signal\", \"options\"];\n  }\n\n  get lc_secrets() {\n    return {\n      zhipuAIApiKey: \"ZHIPUAI_API_KEY\",\n      apiKey: \"ZHIPUAI_API_KEY\",\n    };\n  }\n\n  get lc_aliases() {\n    return undefined;\n  }\n\n  zhipuAIApiKey?: string;\n\n  apiKey?: string;\n\n  streaming: boolean;\n\n  doSample?: boolean;\n\n  messages?: ZhipuMessage[];\n\n  requestId?: string;\n\n  modelName: ChatCompletionRequest[\"model\"];\n\n  model: ChatCompletionRequest[\"model\"];\n\n  apiUrl: string;\n\n  maxTokens?: number | undefined;\n\n  temperature?: number | undefined;\n\n  topP?: number | undefined;\n\n  stop?: string[];\n\n  constructor(fields: Partial<ChatZhipuAIParams> & BaseChatModelParams = {}) {\n    super(fields);\n\n    this.zhipuAIApiKey =\n      fields?.apiKey ??\n      fields?.zhipuAIApiKey ??\n      getEnvironmentVariable(\"ZHIPUAI_API_KEY\");\n    if (!this.zhipuAIApiKey) {\n      throw new Error(\"ZhipuAI API key not found\");\n    }\n\n    this.apiUrl = \"https://open.bigmodel.cn/api/paas/v4/chat/completions\";\n    this.streaming = fields.streaming ?? false;\n    this.messages = fields.messages ?? [];\n    this.temperature = fields.temperature ?? 0.95;\n    this.topP = fields.topP ?? 0.7;\n    this.stop = fields.stop;\n    this.maxTokens = fields.maxTokens;\n    this.modelName = fields?.model ?? fields.modelName ?? \"glm-3-turbo\";\n    this.model = this.modelName;\n    this.doSample = fields.doSample;\n  }\n\n  /**\n   * Get the parameters used to invoke the model\n   */\n  invocationParams(\n    options?: this[\"ParsedCallOptions\"]\n  ): Omit<ChatCompletionRequest, \"messages\"> {\n    return {\n      model: this.model,\n      request_id: this.requestId,\n      do_sample: this.doSample,\n      stream: this.streaming,\n      temperature: this.temperature,\n      top_p: this.topP,\n      max_tokens: this.maxTokens,\n      stop: this.stop,\n      tools: options?.tools?.map((tool) => convertToOpenAITool(tool)) ?? [],\n    };\n  }\n\n  /**\n   * Get the identifying parameters for the model\n   */\n  identifyingParams(): Omit<ChatCompletionRequest, \"messages\"> {\n    return this.invocationParams();\n  }\n\n  /** @ignore */\n  async _generate(\n    messages: BaseMessage[],\n    options?: this[\"ParsedCallOptions\"],\n    runManager?: CallbackManagerForLLMRun\n  ): Promise<ChatResult> {\n    const parameters = this.invocationParams(options);\n\n    const messagesMapped: ZhipuMessage[] = messages.map((message) => ({\n      role: messageToRole(message),\n      content: message.content as string,\n    }));\n\n    const data = parameters.stream\n      ? await new Promise<ChatCompletionResponse>((resolve, reject) => {\n          let response: ChatCompletionResponse;\n          let rejected = false;\n          let resolved = false;\n          this.completionWithRetry(\n            {\n              ...parameters,\n              messages: messagesMapped,\n            },\n            true,\n            options?.signal,\n            (event) => {\n              const data: ChatCompletionResponse = JSON.parse(event.data);\n              if (data?.error?.code) {\n                if (rejected) {\n                  return;\n                }\n                rejected = true;\n                reject(new Error(data?.error?.message));\n                return;\n              }\n\n              const { delta, finish_reason } = data.choices[0];\n              const text = delta.content ?? \"\";\n              const tool_calls = delta.tool_calls ?? [];\n\n              if (!response) {\n                response = {\n                  ...data,\n                  output: { text, finish_reason, tool_calls },\n                };\n              } else {\n                response.output.text += text;\n                response.output.finish_reason = finish_reason;\n                response.output.tool_calls =\n                  response.output.tool_calls?.concat(tool_calls) ?? tool_calls;\n                response.usage = data.usage;\n              }\n\n              // eslint-disable-next-line no-void\n              void runManager?.handleLLMNewToken(text ?? \"\");\n              if (finish_reason && finish_reason !== \"null\") {\n                if (resolved || rejected) return;\n                resolved = true;\n                resolve(response);\n              }\n            }\n          ).catch((error) => {\n            if (!rejected) {\n              rejected = true;\n              reject(error);\n            }\n          });\n        })\n      : await this.completionWithRetry(\n          {\n            ...parameters,\n            messages: messagesMapped,\n          },\n          false,\n          options?.signal\n        ).then<ChatCompletionResponse>((data: ChatCompletionResponse) => {\n          if (data?.error?.code) {\n            throw new Error(data?.error?.message);\n          }\n          const { finish_reason, message } = data.choices[0];\n          const text = message.content ?? \"\";\n          return {\n            ...data,\n            output: { text, finish_reason, tool_calls: message.tool_calls },\n          };\n        });\n\n    const {\n      prompt_tokens = 0,\n      completion_tokens = 0,\n      total_tokens = 0,\n    } = data.usage;\n\n    const { text, tool_calls: rawToolCalls } = data.output;\n    const { toolCalls, invalidToolCalls } = parseRawToolCalls(\n      rawToolCalls ?? []\n    );\n\n    return {\n      generations: [\n        {\n          text,\n          message: new AIMessage({\n            content: text,\n            tool_calls: toolCalls,\n            invalid_tool_calls: invalidToolCalls,\n          }),\n        },\n      ],\n      llmOutput: {\n        tokenUsage: {\n          promptTokens: prompt_tokens,\n          completionTokens: completion_tokens,\n          totalTokens: total_tokens,\n        },\n      },\n    };\n  }\n\n  bindTools(\n    tools: BindToolsInput[],\n    kwargs?: Partial<this[\"ParsedCallOptions\"]>\n  ): Runnable<\n    BaseLanguageModelInput,\n    AIMessageChunk,\n    BaseChatModelCallOptions\n  > {\n    return this.withConfig({\n      tools: tools.map((tool) => convertToOpenAITool(tool)),\n      ...kwargs,\n    });\n  }\n\n  /** @ignore */\n  async completionWithRetry(\n    request: ChatCompletionRequest,\n    stream: boolean,\n    signal?: AbortSignal,\n    onmessage?: (event: MessageEvent) => void\n  ) {\n    const makeCompletionRequest = async () => {\n      const response = await fetch(this.apiUrl, {\n        method: \"POST\",\n        headers: {\n          ...(stream ? { Accept: \"text/event-stream\" } : {}),\n          Authorization: `Bearer ${encodeApiKey(this.zhipuAIApiKey)}`,\n          \"Content-Type\": \"application/json\",\n        },\n        body: JSON.stringify(request),\n        signal,\n      });\n\n      if (!stream) {\n        return response.json();\n      }\n\n      if (response.body) {\n        // response will not be a stream if an error occurred\n        if (\n          !response.headers.get(\"content-type\")?.startsWith(\"text/event-stream\")\n        ) {\n          onmessage?.(\n            new MessageEvent(\"message\", {\n              data: await response.text(),\n            })\n          );\n          return;\n        }\n        const reader = response.body.getReader();\n        const decoder = new TextDecoder(\"utf-8\");\n        let data = \"\";\n        let continueReading = true;\n        while (continueReading) {\n          const { done, value } = await reader.read();\n          if (done) {\n            continueReading = false;\n            break;\n          }\n          data += decoder.decode(value);\n          let continueProcessing = true;\n          while (continueProcessing) {\n            const newlineIndex = data.indexOf(\"\\n\");\n            if (newlineIndex === -1) {\n              continueProcessing = false;\n              break;\n            }\n            const line = data.slice(0, newlineIndex);\n            data = data.slice(newlineIndex + 1);\n            if (line.startsWith(\"data:\")) {\n              const value = line.slice(\"data:\".length).trim();\n              if (value === \"[DONE]\") {\n                continueReading = false;\n                break;\n              }\n              const event = new MessageEvent(\"message\", { data: value });\n              onmessage?.(event);\n            }\n          }\n        }\n      }\n    };\n\n    return this.caller.call(makeCompletionRequest);\n  }\n\n  private async createZhipuStream(\n    request: ChatCompletionRequest,\n    signal?: AbortSignal\n  ) {\n    const response = await fetch(this.apiUrl, {\n      method: \"POST\",\n      headers: {\n        Accept: \"text/event-stream\",\n        Authorization: `Bearer ${encodeApiKey(this.zhipuAIApiKey)}`,\n        \"Content-Type\": \"application/json\",\n      },\n      body: JSON.stringify(request),\n      signal,\n    });\n\n    if (!response.body) {\n      throw new Error(\n        \"Could not begin Zhipu stream. Please check the given URL and try again.\"\n      );\n    }\n\n    return convertEventStreamToIterableReadableDataStream(response.body);\n  }\n\n  private _deserialize(json: string) {\n    try {\n      return JSON.parse(json);\n    } catch {\n      console.warn(`Received a non-JSON parseable chunk: ${json}`);\n    }\n  }\n\n  async *_streamResponseChunks(\n    messages: BaseMessage[],\n    options?: this[\"ParsedCallOptions\"],\n    runManager?: CallbackManagerForLLMRun\n  ): AsyncGenerator<ChatGenerationChunk> {\n    const parameters = {\n      ...this.invocationParams(options),\n      stream: true,\n    };\n\n    const messagesMapped: ZhipuMessage[] = messages.map((message) => ({\n      role: messageToRole(message),\n      content: message.content as string,\n    }));\n\n    const stream = await this.caller.call(async () =>\n      this.createZhipuStream(\n        {\n          ...parameters,\n          messages: messagesMapped,\n        },\n        options?.signal\n      )\n    );\n\n    for await (const chunk of stream) {\n      if (chunk !== \"[DONE]\") {\n        const deserializedChunk = this._deserialize(chunk);\n        const { choices, usage, id } = deserializedChunk;\n        const text = choices[0]?.delta?.content ?? \"\";\n        const rawToolCalls = choices[0]?.delta?.tool_calls ?? [];\n        const { toolCalls, invalidToolCalls } = parseRawToolCalls(rawToolCalls);\n        const finished = !!choices[0]?.finish_reason;\n        const isToolCall = rawToolCalls.length > 0;\n        yield new ChatGenerationChunk({\n          text,\n          // In stream mode, ZhipuAI's delta chunk includes\n          // either `tool_calls` or `content` field, but not both\n          message: isToolCall\n            ? new AIMessageChunk({\n                tool_calls: toolCalls,\n                invalid_tool_calls: invalidToolCalls,\n              })\n            : new AIMessageChunk({ content: text }),\n          generationInfo: finished\n            ? {\n                finished,\n                request_id: id,\n                // The usage of tokens is counted at the end of the stream\n                usage,\n              }\n            : undefined,\n        });\n        await runManager?.handleLLMNewToken(text);\n      } else {\n        continue;\n      }\n    }\n  }\n\n  _llmType(): string {\n    return \"zhipuai\";\n  }\n\n  /** @ignore */\n  _combineLLMOutput() {\n    return [];\n  }\n}\n"],"mappings":";;;;;;;;;;;;;AA6LA,SAAS,cAAcA,SAAwC;CAC7D,MAAM,OAAO,QAAQ,UAAU;AAC/B,SAAQ,MAAR;EACE,KAAK,KACH,QAAO;EACT,KAAK,QACH,QAAO;EACT,KAAK,SACH,QAAO;EACT,KAAK,WACH,OAAM,IAAI,MAAM;EAClB,KAAK;AACH,OAAI,CAAC,YAAY,WAAW,QAAQ,CAClC,OAAM,IAAI,MAAM;AAElB,OAAI;IAAC;IAAU;IAAa;GAAO,EAAC,SAAS,QAAQ,KAAK,CACxD,QAAO,QAAQ;AAEjB,SAAM,IAAI,MAAM,CAAC,sBAAsB,EAAE,MAAM;EAEjD,QACE,OAAM,IAAI,MAAM,CAAC,sBAAsB,EAAE,MAAM;CAClD;AACF;AAED,SAAS,kBAAkBC,cAA0B;CACnD,MAAM,YAAY,CAAE;CACpB,MAAM,mBAAmB,CAAE;AAC3B,MAAK,MAAM,eAAe,aACxB,KAAI;EACF,UAAU,KAAK,cAAc,aAAa,EAAE,UAAU,KAAM,EAAC,CAAC;CAE/D,SAAQC,GAAQ;EACf,iBAAiB,KAAK,oBAAoB,aAAa,EAAE,QAAQ,CAAC;CACnE;AAEH,QAAO;EAAE;EAAW;CAAkB;AACvC;AAED,IAAa,cAAb,cACU,cAEV;CACE,OAAO,UAAU;AACf,SAAO;CACR;CAED,IAAI,WAAW;AACb,SAAO;GAAC;GAAQ;GAAU;EAAU;CACrC;CAED,IAAI,aAAa;AACf,SAAO;GACL,eAAe;GACf,QAAQ;EACT;CACF;CAED,IAAI,aAAa;AACf,SAAO;CACR;CAED;CAEA;CAEA;CAEA;CAEA;CAEA;CAEA;CAEA;CAEA;CAEA;CAEA;CAEA;CAEA;CAEA,YAAYC,SAA2D,CAAE,GAAE;EACzE,MAAM,OAAO;EAEb,KAAK,gBACH,QAAQ,UACR,QAAQ,iBACR,uBAAuB,kBAAkB;AAC3C,MAAI,CAAC,KAAK,cACR,OAAM,IAAI,MAAM;EAGlB,KAAK,SAAS;EACd,KAAK,YAAY,OAAO,aAAa;EACrC,KAAK,WAAW,OAAO,YAAY,CAAE;EACrC,KAAK,cAAc,OAAO,eAAe;EACzC,KAAK,OAAO,OAAO,QAAQ;EAC3B,KAAK,OAAO,OAAO;EACnB,KAAK,YAAY,OAAO;EACxB,KAAK,YAAY,QAAQ,SAAS,OAAO,aAAa;EACtD,KAAK,QAAQ,KAAK;EAClB,KAAK,WAAW,OAAO;CACxB;;;;CAKD,iBACEC,SACyC;AACzC,SAAO;GACL,OAAO,KAAK;GACZ,YAAY,KAAK;GACjB,WAAW,KAAK;GAChB,QAAQ,KAAK;GACb,aAAa,KAAK;GAClB,OAAO,KAAK;GACZ,YAAY,KAAK;GACjB,MAAM,KAAK;GACX,OAAO,SAAS,OAAO,IAAI,CAAC,SAAS,oBAAoB,KAAK,CAAC,IAAI,CAAE;EACtE;CACF;;;;CAKD,oBAA6D;AAC3D,SAAO,KAAK,kBAAkB;CAC/B;;CAGD,MAAM,UACJC,UACAD,SACAE,YACqB;EACrB,MAAM,aAAa,KAAK,iBAAiB,QAAQ;EAEjD,MAAMC,iBAAiC,SAAS,IAAI,CAAC,aAAa;GAChE,MAAM,cAAc,QAAQ;GAC5B,SAAS,QAAQ;EAClB,GAAE;EAEH,MAAM,OAAO,WAAW,SACpB,MAAM,IAAI,QAAgC,CAAC,SAAS,WAAW;GAC7D,IAAIC;GACJ,IAAI,WAAW;GACf,IAAI,WAAW;GACf,KAAK,oBACH;IACE,GAAG;IACH,UAAU;GACX,GACD,MACA,SAAS,QACT,CAAC,UAAU;IACT,MAAMC,SAA+B,KAAK,MAAM,MAAM,KAAK;AAC3D,QAAIC,QAAM,OAAO,MAAM;AACrB,SAAI,SACF;KAEF,WAAW;KACX,OAAO,IAAI,MAAMA,QAAM,OAAO,SAAS;AACvC;IACD;IAED,MAAM,EAAE,OAAO,eAAe,GAAGA,OAAK,QAAQ;IAC9C,MAAMC,SAAO,MAAM,WAAW;IAC9B,MAAM,aAAa,MAAM,cAAc,CAAE;AAEzC,QAAI,CAAC,UACH,WAAW;KACT,GAAGD;KACH,QAAQ;MAAE;MAAM;MAAe;KAAY;IAC5C;SACI;KACL,SAAS,OAAO,QAAQC;KACxB,SAAS,OAAO,gBAAgB;KAChC,SAAS,OAAO,aACd,SAAS,OAAO,YAAY,OAAO,WAAW,IAAI;KACpD,SAAS,QAAQD,OAAK;IACvB;IAGI,YAAY,kBAAkBC,UAAQ,GAAG;AAC9C,QAAI,iBAAiB,kBAAkB,QAAQ;AAC7C,SAAI,YAAY,SAAU;KAC1B,WAAW;KACX,QAAQ,SAAS;IAClB;GACF,EACF,CAAC,MAAM,CAAC,UAAU;AACjB,QAAI,CAAC,UAAU;KACb,WAAW;KACX,OAAO,MAAM;IACd;GACF,EAAC;EACH,KACD,MAAM,KAAK,oBACT;GACE,GAAG;GACH,UAAU;EACX,GACD,OACA,SAAS,OACV,CAAC,KAA6B,CAACF,WAAiC;AAC/D,OAAIC,QAAM,OAAO,KACf,OAAM,IAAI,MAAMA,QAAM,OAAO;GAE/B,MAAM,EAAE,eAAe,SAAS,GAAGA,OAAK,QAAQ;GAChD,MAAMC,SAAO,QAAQ,WAAW;AAChC,UAAO;IACL,GAAGD;IACH,QAAQ;KAAE;KAAM;KAAe,YAAY,QAAQ;IAAY;GAChE;EACF,EAAC;EAEN,MAAM,EACJ,gBAAgB,GAChB,oBAAoB,GACpB,eAAe,GAChB,GAAG,KAAK;EAET,MAAM,EAAE,MAAM,YAAY,cAAc,GAAG,KAAK;EAChD,MAAM,EAAE,WAAW,kBAAkB,GAAG,kBACtC,gBAAgB,CAAE,EACnB;AAED,SAAO;GACL,aAAa,CACX;IACE;IACA,SAAS,IAAI,UAAU;KACrB,SAAS;KACT,YAAY;KACZ,oBAAoB;IACrB;GACF,CACF;GACD,WAAW,EACT,YAAY;IACV,cAAc;IACd,kBAAkB;IAClB,aAAa;GACd,EACF;EACF;CACF;CAED,UACEE,OACAC,QAKA;AACA,SAAO,KAAK,WAAW;GACrB,OAAO,MAAM,IAAI,CAAC,SAAS,oBAAoB,KAAK,CAAC;GACrD,GAAG;EACJ,EAAC;CACH;;CAGD,MAAM,oBACJC,SACAC,QACAC,QACAC,WACA;EACA,MAAM,wBAAwB,YAAY;GACxC,MAAM,WAAW,MAAM,MAAM,KAAK,QAAQ;IACxC,QAAQ;IACR,SAAS;KACP,GAAI,SAAS,EAAE,QAAQ,oBAAqB,IAAG,CAAE;KACjD,eAAe,CAAC,OAAO,EAAE,aAAa,KAAK,cAAc,EAAE;KAC3D,gBAAgB;IACjB;IACD,MAAM,KAAK,UAAU,QAAQ;IAC7B;GACD,EAAC;AAEF,OAAI,CAAC,OACH,QAAO,SAAS,MAAM;AAGxB,OAAI,SAAS,MAAM;AAEjB,QACE,CAAC,SAAS,QAAQ,IAAI,eAAe,EAAE,WAAW,oBAAoB,EACtE;KACA,YACE,IAAI,aAAa,WAAW,EAC1B,MAAM,MAAM,SAAS,MAAM,CAC5B,GACF;AACD;IACD;IACD,MAAM,SAAS,SAAS,KAAK,WAAW;IACxC,MAAM,UAAU,IAAI,YAAY;IAChC,IAAI,OAAO;IACX,IAAI,kBAAkB;AACtB,WAAO,iBAAiB;KACtB,MAAM,EAAE,MAAM,OAAO,GAAG,MAAM,OAAO,MAAM;AAC3C,SAAI,MAAM;MACR,kBAAkB;AAClB;KACD;KACD,QAAQ,QAAQ,OAAO,MAAM;KAC7B,IAAI,qBAAqB;AACzB,YAAO,oBAAoB;MACzB,MAAM,eAAe,KAAK,QAAQ,KAAK;AACvC,UAAI,iBAAiB,IAAI;OACvB,qBAAqB;AACrB;MACD;MACD,MAAM,OAAO,KAAK,MAAM,GAAG,aAAa;MACxC,OAAO,KAAK,MAAM,eAAe,EAAE;AACnC,UAAI,KAAK,WAAW,QAAQ,EAAE;OAC5B,MAAMC,UAAQ,KAAK,MAAM,EAAe,CAAC,MAAM;AAC/C,WAAIA,YAAU,UAAU;QACtB,kBAAkB;AAClB;OACD;OACD,MAAM,QAAQ,IAAI,aAAa,WAAW,EAAE,MAAMA,QAAO;OACzD,YAAY,MAAM;MACnB;KACF;IACF;GACF;EACF;AAED,SAAO,KAAK,OAAO,KAAK,sBAAsB;CAC/C;CAED,MAAc,kBACZJ,SACAE,QACA;EACA,MAAM,WAAW,MAAM,MAAM,KAAK,QAAQ;GACxC,QAAQ;GACR,SAAS;IACP,QAAQ;IACR,eAAe,CAAC,OAAO,EAAE,aAAa,KAAK,cAAc,EAAE;IAC3D,gBAAgB;GACjB;GACD,MAAM,KAAK,UAAU,QAAQ;GAC7B;EACD,EAAC;AAEF,MAAI,CAAC,SAAS,KACZ,OAAM,IAAI,MACR;AAIJ,SAAO,+CAA+C,SAAS,KAAK;CACrE;CAED,AAAQ,aAAaG,MAAc;AACjC,MAAI;AACF,UAAO,KAAK,MAAM,KAAK;EACxB,QAAO;GACN,QAAQ,KAAK,CAAC,qCAAqC,EAAE,MAAM,CAAC;EAC7D;CACF;CAED,OAAO,sBACLd,UACAD,SACAE,YACqC;EACrC,MAAM,aAAa;GACjB,GAAG,KAAK,iBAAiB,QAAQ;GACjC,QAAQ;EACT;EAED,MAAMC,iBAAiC,SAAS,IAAI,CAAC,aAAa;GAChE,MAAM,cAAc,QAAQ;GAC5B,SAAS,QAAQ;EAClB,GAAE;EAEH,MAAM,SAAS,MAAM,KAAK,OAAO,KAAK,YACpC,KAAK,kBACH;GACE,GAAG;GACH,UAAU;EACX,GACD,SAAS,OACV,CACF;AAED,aAAW,MAAM,SAAS,OACxB,KAAI,UAAU,UAAU;GACtB,MAAM,oBAAoB,KAAK,aAAa,MAAM;GAClD,MAAM,EAAE,SAAS,OAAO,IAAI,GAAG;GAC/B,MAAM,OAAO,QAAQ,IAAI,OAAO,WAAW;GAC3C,MAAM,eAAe,QAAQ,IAAI,OAAO,cAAc,CAAE;GACxD,MAAM,EAAE,WAAW,kBAAkB,GAAG,kBAAkB,aAAa;GACvE,MAAM,WAAW,CAAC,CAAC,QAAQ,IAAI;GAC/B,MAAM,aAAa,aAAa,SAAS;GACzC,MAAM,IAAI,oBAAoB;IAC5B;IAGA,SAAS,aACL,IAAI,eAAe;KACjB,YAAY;KACZ,oBAAoB;IACrB,KACD,IAAI,eAAe,EAAE,SAAS,KAAM;IACxC,gBAAgB,WACZ;KACE;KACA,YAAY;KAEZ;IACD,IACD;GACL;GACD,MAAM,YAAY,kBAAkB,KAAK;EAC1C,MACC;CAGL;CAED,WAAmB;AACjB,SAAO;CACR;;CAGD,oBAAoB;AAClB,SAAO,CAAE;CACV;AACF"}