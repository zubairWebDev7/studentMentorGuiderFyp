{"version":3,"file":"config.d.ts","names":["BaseLanguageModel","RunnableConfig","Example","Run","EvaluationResult","RunEvaluator","Criteria","CriteriaType","EmbeddingDistanceEvalChainInput","LoadEvaluatorOptions","EvaluatorType","EvaluatorInputs","EvaluatorInputFormatter","rawInput","rawPrediction","rawReferenceOutput","run","DynamicRunEvaluatorParams","Input","Prediction","Reference","Record","RunEvaluatorLike","Promise","isOffTheShelfEvaluator","T","U","EvalConfig","isCustomEvaluator","RunEvalType","RunEvalConfig","CriteriaEvalChainConfig","LabeledCriteria","Partial","Pick","EmbeddingDistance"],"sources":["../../src/smith/config.d.ts"],"sourcesContent":["import { BaseLanguageModel } from \"@langchain/core/language_models/base\";\nimport { RunnableConfig } from \"@langchain/core/runnables\";\nimport { Example, Run } from \"langsmith\";\nimport { EvaluationResult, RunEvaluator } from \"langsmith/evaluation\";\nimport { Criteria as CriteriaType, type EmbeddingDistanceEvalChainInput } from \"../evaluation/index.js\";\nimport { LoadEvaluatorOptions } from \"../evaluation/loader.js\";\nimport { EvaluatorType } from \"../evaluation/types.js\";\nexport type EvaluatorInputs = {\n    input?: string | unknown;\n    prediction: string | unknown;\n    reference?: string | unknown;\n};\nexport type EvaluatorInputFormatter = ({ rawInput, rawPrediction, rawReferenceOutput, run }: {\n    rawInput: any;\n    rawPrediction: any;\n    rawReferenceOutput?: any;\n    run: Run;\n}) => EvaluatorInputs;\nexport type DynamicRunEvaluatorParams<Input extends Record<string, any> = Record<string, unknown>, Prediction extends Record<string, any> = Record<string, unknown>, Reference extends Record<string, any> = Record<string, unknown>> = {\n    input: Input;\n    prediction?: Prediction;\n    reference?: Reference;\n    run: Run;\n    example?: Example;\n};\n/**\n * Type of a function that can be coerced into a RunEvaluator function.\n * While we have the class-based RunEvaluator, it's often more convenient to directly\n * pass a function to the runner. This type allows us to do that.\n */\nexport type RunEvaluatorLike = ((props: DynamicRunEvaluatorParams, options: RunnableConfig) => Promise<EvaluationResult>) | ((props: DynamicRunEvaluatorParams, options: RunnableConfig) => EvaluationResult);\nexport declare function isOffTheShelfEvaluator<T extends keyof EvaluatorType, U extends RunEvaluator | RunEvaluatorLike = RunEvaluator | RunEvaluatorLike>(evaluator: T | EvalConfig | U): evaluator is T | EvalConfig;\nexport declare function isCustomEvaluator<T extends keyof EvaluatorType, U extends RunEvaluator | RunEvaluatorLike = RunEvaluator | RunEvaluatorLike>(evaluator: T | EvalConfig | U): evaluator is U;\nexport type RunEvalType<T extends keyof EvaluatorType = \"criteria\" | \"labeled_criteria\" | \"embedding_distance\", U extends RunEvaluator | RunEvaluatorLike = RunEvaluator | RunEvaluatorLike> = T | EvalConfig | U;\n/**\n * Configuration class for running evaluations on datasets.\n *\n * @remarks\n * RunEvalConfig in LangSmith is a configuration class for running evaluations on datasets. Its primary purpose is to define the parameters and evaluators that will be applied during the evaluation of a dataset. This configuration can include various evaluators, custom evaluators, and different keys for inputs, predictions, and references.\n *\n * @typeparam T - The type of evaluators.\n * @typeparam U - The type of custom evaluators.\n */\nexport type RunEvalConfig<T extends keyof EvaluatorType = \"criteria\" | \"labeled_criteria\" | \"embedding_distance\", U extends RunEvaluator | RunEvaluatorLike = RunEvaluator | RunEvaluatorLike> = {\n    /**\n     * Evaluators to apply to a dataset run.\n     * You can optionally specify these by name, or by\n     * configuring them with an EvalConfig object.\n     */\n    evaluators?: RunEvalType<T, U>[];\n    /**\n     * Convert the evaluation data into formats that can be used by the evaluator.\n     * This should most commonly be a string.\n     * Parameters are the raw input from the run, the raw output, raw reference output, and the raw run.\n     * @example\n     * ```ts\n     * // Chain input: { input: \"some string\" }\n     * // Chain output: { output: \"some output\" }\n     * // Reference example output format: { output: \"some reference output\" }\n     * const formatEvaluatorInputs = ({\n     *   rawInput,\n     *   rawPrediction,\n     *   rawReferenceOutput,\n     * }) => {\n     *   return {\n     *     input: rawInput.input,\n     *     prediction: rawPrediction.output,\n     *     reference: rawReferenceOutput.output,\n     *   };\n     * };\n     * ```\n     * @returns The prepared data.\n     */\n    formatEvaluatorInputs?: EvaluatorInputFormatter;\n    /**\n     * Custom evaluators to apply to a dataset run.\n     * Each evaluator is provided with a run trace containing the model\n     * outputs, as well as an \"example\" object representing a record\n     * in the dataset.\n     *\n     * @deprecated Use `evaluators` instead.\n     */\n    customEvaluators?: U[];\n};\nexport interface EvalConfig extends LoadEvaluatorOptions {\n    /**\n     * The name of the evaluator to use.\n     * Example: labeled_criteria, criteria, etc.\n     */\n    evaluatorType: keyof EvaluatorType;\n    /**\n     * The feedback (or metric) name to use for the logged\n     * evaluation results. If none provided, we default to\n     * the evaluationName.\n     */\n    feedbackKey?: string;\n    /**\n     * Convert the evaluation data into formats that can be used by the evaluator.\n     * This should most commonly be a string.\n     * Parameters are the raw input from the run, the raw output, raw reference output, and the raw run.\n     * @example\n     * ```ts\n     * // Chain input: { input: \"some string\" }\n     * // Chain output: { output: \"some output\" }\n     * // Reference example output format: { output: \"some reference output\" }\n     * const formatEvaluatorInputs = ({\n     *   rawInput,\n     *   rawPrediction,\n     *   rawReferenceOutput,\n     * }) => {\n     *   return {\n     *     input: rawInput.input,\n     *     prediction: rawPrediction.output,\n     *     reference: rawReferenceOutput.output,\n     *   };\n     * };\n     * ```\n     * @returns The prepared data.\n     */\n    formatEvaluatorInputs: EvaluatorInputFormatter;\n}\n/**\n * Configuration to load a \"CriteriaEvalChain\" evaluator,\n * which prompts an LLM to determine whether the model's\n * prediction complies with the provided criteria.\n * @param criteria - The criteria to use for the evaluator.\n * @param llm - The language model to use for the evaluator.\n * @returns The configuration for the evaluator.\n * @example\n * ```ts\n * const evalConfig = {\n *   evaluators: [Criteria(\"helpfulness\")],\n * };\n * @example\n * ```ts\n * const evalConfig = {\n *   evaluators: [\n *     Criteria({\n *       \"isCompliant\": \"Does the submission comply with the requirements of XYZ\"\n *     })\n *   ],\n * };\n * @example\n * ```ts\n * const evalConfig = {\n *   evaluators: [{\n *     evaluatorType: \"criteria\",\n *     criteria: \"helpfulness\"\n *     formatEvaluatorInputs: ...\n *   }]\n * };\n * ```\n * @example\n * ```ts\n * const evalConfig = {\n *   evaluators: [{\n *     evaluatorType: \"criteria\",\n *     criteria: { \"isCompliant\": \"Does the submission comply with the requirements of XYZ\" },\n *     formatEvaluatorInputs: ...\n *   }]\n * };\n */\nexport type Criteria = EvalConfig & {\n    evaluatorType: \"criteria\";\n    /**\n     * The \"criteria\" to insert into the prompt template\n     * used for evaluation. See the prompt at\n     * https://smith.langchain.com/hub/langchain-ai/criteria-evaluator\n     * for more information.\n     */\n    criteria?: CriteriaType | Record<string, string>;\n    /**\n     * The language model to use as the evaluator, defaults to GPT-4\n     */\n    llm?: BaseLanguageModel;\n};\nexport type CriteriaEvalChainConfig = Criteria;\nexport declare function Criteria(criteria: CriteriaType | Record<string, string>, config?: Pick<Partial<LabeledCriteria>, \"formatEvaluatorInputs\" | \"llm\" | \"feedbackKey\">): EvalConfig;\n/**\n * Configuration to load a \"LabeledCriteriaEvalChain\" evaluator,\n * which prompts an LLM to determine whether the model's\n * prediction complies with the provided criteria and also\n * provides a \"ground truth\" label for the evaluator to incorporate\n * in its evaluation.\n * @param criteria - The criteria to use for the evaluator.\n * @param llm - The language model to use for the evaluator.\n * @returns The configuration for the evaluator.\n * @example\n * ```ts\n * const evalConfig = {\n *   evaluators: [LabeledCriteria(\"correctness\")],\n * };\n * @example\n * ```ts\n * const evalConfig = {\n *   evaluators: [\n *     LabeledCriteria({\n *       \"mentionsAllFacts\": \"Does the include all facts provided in the reference?\"\n *     })\n *   ],\n * };\n * @example\n * ```ts\n * const evalConfig = {\n *   evaluators: [{\n *     evaluatorType: \"labeled_criteria\",\n *     criteria: \"correctness\",\n *     formatEvaluatorInputs: ...\n *   }],\n * };\n * ```\n * @example\n * ```ts\n * const evalConfig = {\n *   evaluators: [{\n *     evaluatorType: \"labeled_criteria\",\n *     criteria: { \"mentionsAllFacts\": \"Does the include all facts provided in the reference?\" },\n *     formatEvaluatorInputs: ...\n *   }],\n * };\n */\nexport type LabeledCriteria = EvalConfig & {\n    evaluatorType: \"labeled_criteria\";\n    /**\n     * The \"criteria\" to insert into the prompt template\n     * used for evaluation. See the prompt at\n     * https://smith.langchain.com/hub/langchain-ai/labeled-criteria\n     * for more information.\n     */\n    criteria?: CriteriaType | Record<string, string>;\n    /**\n     * The language model to use as the evaluator, defaults to GPT-4\n     */\n    llm?: BaseLanguageModel;\n};\nexport declare function LabeledCriteria(criteria: CriteriaType | Record<string, string>, config?: Pick<Partial<LabeledCriteria>, \"formatEvaluatorInputs\" | \"llm\" | \"feedbackKey\">): LabeledCriteria;\n/**\n * Configuration to load a \"EmbeddingDistanceEvalChain\" evaluator,\n * which embeds distances to score semantic difference between\n * a prediction and reference.\n */\nexport type EmbeddingDistance = EvalConfig & EmbeddingDistanceEvalChainInput & {\n    evaluatorType: \"embedding_distance\";\n};\nexport declare function EmbeddingDistance(distanceMetric: EmbeddingDistanceEvalChainInput[\"distanceMetric\"], config?: Pick<Partial<LabeledCriteria>, \"formatEvaluatorInputs\" | \"embedding\" | \"feedbackKey\">): EmbeddingDistance;\n//# sourceMappingURL=config.d.ts.map"],"mappings":";;;;;;;;;;;KAOYW,eAAAA;;;;AAAZ,CAAA;AAKYC,KAAAA,uBAAAA,GAAuB,CAAA;EAAA,QAAA;EAAA,aAAA;EAAA,kBAAA;EAAA;CAAA,EAAA;EAAMC,QAAAA,EAAAA,GAAAA;EAAUC,aAAAA,EAAAA,GAAAA;EAAeC,kBAAAA,CAAAA,EAAAA,GAAAA;EAAoBC,GAAAA,EAI7Eb,GAJ6Ea;CAI7Eb,EAAAA,GACHQ,eADGR;AACHQ,KACMM,yBADNN,CAAAA,cAC8CU,MAD9CV,CAAAA,MAAAA,EAAAA,GAAAA,CAAAA,GACoEU,MADpEV,CAAAA,MAAAA,EAAAA,OAAAA,CAAAA,EAAAA,mBACgHU,MADhHV,CAAAA,MAAAA,EAAAA,GAAAA,CAAAA,GACsIU,MADtIV,CAAAA,MAAAA,EAAAA,OAAAA,CAAAA,EAAAA,kBACiLU,MADjLV,CAAAA,MAAAA,EAAAA,GAAAA,CAAAA,GACuMU,MADvMV,CAAAA,MAAAA,EAAAA,OAAAA,CAAAA,CAAAA,GAAAA;EAAe,KAAA,EAEVO,KAFU;EACTD,UAAAA,CAAAA,EAEKE,UAFLF;EAAwCI,SAAAA,CAAAA,EAGpCD,SAHoCC;EAAsBA,GAAAA,EAIjElB,GAJiEkB;EAA4CA,OAAAA,CAAAA,EAKxGnB,OALwGmB;CAAsBA;;;;;;AAInIlB,KAQGmB,gBAAAA,GARHnB,CAAAA,CAAAA,KAAAA,EAQ+Bc,yBAR/Bd,EAAAA,OAAAA,EAQmEF,cARnEE,EAAAA,GAQsFoB,OARtFpB,CAQ8FC,gBAR9FD,CAAAA,CAAAA,GAAAA,CAAAA,CAAAA,KAAAA,EAQ4Hc,yBAR5Hd,EAAAA,OAAAA,EAQgKF,cARhKE,EAAAA,GAQmLC,gBARnLD,CAAAA;AACKD,iBAQUsB,sBARVtB,CAAAA,UAAAA,MAQiDQ,aARjDR,EAAAA,UAQ0EG,YAR1EH,GAQyFoB,gBARzFpB,GAQ4GG,YAR5GH,GAQ2HoB,gBAR3HpB,CAAAA,CAAAA,SAAAA,EAQwJuB,CARxJvB,GAQ4JyB,UAR5JzB,GAQyKwB,CARzKxB,CAAAA,EAAAA,SAAAA,IAQ0LuB,CAR1LvB,GAQ8LyB,UAR9LzB;AAAO,iBASG0B,iBATH,CAAA,UAAA,MASqClB,aATrC,EAAA,UAS8DL,YAT9D,GAS6EiB,gBAT7E,GASgGjB,YAThG,GAS+GiB,gBAT/G,CAAA,CAAA,SAAA,EAS4IG,CAT5I,GASgJE,UAThJ,GAS6JD,CAT7J,CAAA,EAAA,SAAA,IAS8KA,CAT9K;AAOTJ,KAGAO,WAHgB,CAAA,UAAA,MAGYnB,aAHZ,GAAA,UAAA,GAAA,kBAAA,GAAA,oBAAA,EAAA,UAG8FL,YAH9F,GAG6GiB,gBAH7G,GAGgIjB,YAHhI,GAG+IiB,gBAH/I,CAAA,GAGmKG,CAHnK,GAGuKE,UAHvK,GAGoLD,CAHpL;;;;;;;;AAAgL;AAC5M;AAA+DhB,KAYnDoB,aAZmDpB,CAAAA,UAAAA,MAYrBA,aAZqBA,GAAAA,UAAAA,GAAAA,kBAAAA,GAAAA,oBAAAA,EAAAA,UAY6DL,YAZ7DK,GAY4EY,gBAZ5EZ,GAY+FL,YAZ/FK,GAY8GY,gBAZ9GZ,CAAAA,GAAAA;EAAyBL;;;;;EAAkFsB,UAAAA,CAAAA,EAkBzJE,WAlByJF,CAkB7IF,CAlB6IE,EAkB1ID,CAlB0IC,CAAAA,EAAAA;EAAaD;;;AAA+B;AACtN;;;;;;;;;;AAAoM;AACpM;;;;;;;;EAAgNA,qBAAAA,CAAAA,EAwCpLd,uBAxCoLc;EAAC;AAUjN;;;;;;;EAMgCA,gBAAAA,CAAAA,EAiCTA,CAjCSA,EAAAA;CAAfG;AAwBWjB,UAWXe,UAAAA,SAAmBlB,oBAXRG,CAAAA;EASLc;AAAC;AAExB;;EAmC2Bd,aAAAA,EAAAA,MA9BFF,aA8BEE;EAnCSH;AAAoB;AA8ExD;;;EAQ8BY,WAAAA,CAAAA,EAAAA,MAAAA;EAIpBrB;AAAiB;AAE3B;AACA;;;;;;;AAAuL;AA4CvL;;;;;AAY2B;AAE3B;;;;;;EAAoLgC,qBAAAA,EApHzJpB,uBAoHyJoB;AAAe;AAMnM;AAGA;;;;;;AAA+N;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;KAlFnN1B,UAAAA,GAAWqB;;;;;;;;aAQRpB,WAAec;;;;QAIpBrB;;KAEE+B,uBAAAA,GAA0BzB;iBACdA,UAAAA,WAAmBC,WAAec,iCAAiCa,KAAKD,QAAQD,qEAAqEL;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;KA4CjKK,eAAAA,GAAkBL;;;;;;;;aAQfpB,WAAec;;;;QAIpBrB;;iBAEcgC,eAAAA,WAA0BzB,WAAec,iCAAiCa,KAAKD,QAAQD,qEAAqEA;;;;;;KAMxKG,iBAAAA,GAAoBR,aAAanB;;;iBAGrB2B,iBAAAA,iBAAkC3B,4DAA4D0B,KAAKD,QAAQD,2EAA2EG"}