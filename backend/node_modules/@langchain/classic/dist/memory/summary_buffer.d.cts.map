{"version":3,"file":"summary_buffer.d.cts","names":["InputValues","MemoryVariables","OutputValues","BaseConversationSummaryMemory","BaseConversationSummaryMemoryInput","ConversationSummaryBufferMemoryInput","ConversationSummaryBufferMemory","Promise"],"sources":["../../src/memory/summary_buffer.d.ts"],"sourcesContent":["import { InputValues, MemoryVariables, OutputValues } from \"@langchain/core/memory\";\nimport { BaseConversationSummaryMemory, BaseConversationSummaryMemoryInput } from \"./summary.js\";\n/**\n * Interface for the input parameters of the\n * ConversationSummaryBufferMemory class.\n */\nexport interface ConversationSummaryBufferMemoryInput extends BaseConversationSummaryMemoryInput {\n    maxTokenLimit?: number;\n}\n/**\n * Class that extends BaseConversationSummaryMemory and implements\n * ConversationSummaryBufferMemoryInput. It manages the conversation\n * history in a LangChain application by maintaining a buffer of chat\n * messages and providing methods to load, save, prune, and clear the\n * memory.\n * @example\n * ```typescript\n * // Initialize the memory with a specific model and token limit\n * const memory = new ConversationSummaryBufferMemory({\n *   llm: new ChatOpenAI({ model: \"gpt-3.5-turbo-instruct\", temperature: 0 }),\n *   maxTokenLimit: 10,\n * });\n *\n * // Save conversation context to memory\n * await memory.saveContext({ input: \"hi\" }, { output: \"whats up\" });\n * await memory.saveContext({ input: \"not much you\" }, { output: \"not much\" });\n *\n * // Load the conversation history from memory\n * const history = await memory.loadMemoryVariables({});\n * console.log({ history });\n *\n * // Create a chat prompt using the conversation history\n * const chatPrompt = ChatPromptTemplate.fromMessages([\n *   SystemMessagePromptTemplate.fromTemplate(\n *     \"The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\",\n *   ),\n *   new MessagesPlaceholder(\"history\"),\n *   HumanMessagePromptTemplate.fromTemplate(\"{input}\"),\n * ]);\n *\n * // Initialize the conversation chain with the model, memory, and prompt\n * const chain = new ConversationChain({\n *   llm: new ChatOpenAI({ model: \"gpt-4o-mini\", temperature: 0.9, verbose: true }),\n *   memory: memory,\n *   prompt: chatPrompt,\n * });\n * ```\n */\nexport declare class ConversationSummaryBufferMemory extends BaseConversationSummaryMemory implements ConversationSummaryBufferMemoryInput {\n    movingSummaryBuffer: string;\n    maxTokenLimit: number;\n    constructor(fields: ConversationSummaryBufferMemoryInput);\n    get memoryKeys(): string[];\n    /**\n     * Method that loads the chat messages from the memory and returns them as\n     * a string or as a list of messages, depending on the returnMessages\n     * property.\n     * @param _ InputValues object, not used in this method.\n     * @returns Promise that resolves with MemoryVariables object containing the loaded chat messages.\n     */\n    loadMemoryVariables(_?: InputValues): Promise<MemoryVariables>;\n    /**\n     * Method that saves the context of the conversation, including the input\n     * and output values, and prunes the memory if it exceeds the maximum\n     * token limit.\n     * @param inputValues InputValues object containing the input values of the conversation.\n     * @param outputValues OutputValues object containing the output values of the conversation.\n     * @returns Promise that resolves when the context is saved and the memory is pruned.\n     */\n    saveContext(inputValues: InputValues, outputValues: OutputValues): Promise<void>;\n    /**\n     * Method that prunes the memory if the total number of tokens in the\n     * buffer exceeds the maxTokenLimit. It removes messages from the\n     * beginning of the buffer until the total number of tokens is within the\n     * limit.\n     * @returns Promise that resolves when the memory is pruned.\n     */\n    prune(): Promise<void>;\n    /**\n     * Method that clears the memory and resets the movingSummaryBuffer.\n     * @returns Promise that resolves when the memory is cleared.\n     */\n    clear(): Promise<void>;\n}\n//# sourceMappingURL=summary_buffer.d.ts.map"],"mappings":";;;;;;;AAMA;AA0CA;AAGwBK,UA7CPA,oCAAAA,SAA6CD,kCA6CtCC,CAAAA;EASIL,aAAAA,CAAAA,EAAAA,MAAAA;;;;;;;;;;AAZ8G;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;cAArHM,+BAAAA,SAAwCH,6BAAAA,YAAyCE;;;sBAG9EA;;;;;;;;;0BASIL,cAAcO,QAAQN;;;;;;;;;2BASrBD,2BAA2BE,eAAeK;;;;;;;;WAQ1DA;;;;;WAKAA"}