{"version":3,"file":"premai.d.ts","names":["BaseMessage","BaseLanguageModelCallOptions","CallbackManagerForLLMRun","BaseChatModelParams","BaseChatModel","Prem","ChatCompletionStreamingCompletionData","CreateChatCompletionRequest","CreateChatCompletionResponse","ChatGenerationChunk","ChatResult","RoleEnum","ChatPremInput","ChatCompletionCreateParamsNonStreaming","ChatCompletionCreateParamsStreaming","ChatCompletionCreateParams","messageToPremRole","ChatPrem","CallOptions","AsyncIterable","Promise","AsyncGenerator"],"sources":["../../src/chat_models/premai.d.ts"],"sourcesContent":["import { type BaseMessage } from \"@langchain/core/messages\";\nimport { type BaseLanguageModelCallOptions } from \"@langchain/core/language_models/base\";\nimport { CallbackManagerForLLMRun } from \"@langchain/core/callbacks/manager\";\nimport { type BaseChatModelParams, BaseChatModel } from \"@langchain/core/language_models/chat_models\";\nimport Prem, { ChatCompletionStreamingCompletionData, CreateChatCompletionRequest, CreateChatCompletionResponse } from \"@premai/prem-sdk\";\nimport { ChatGenerationChunk, ChatResult } from \"@langchain/core/outputs\";\nexport type RoleEnum = \"user\" | \"assistant\";\n/**\n * Input to chat model class.\n */\nexport interface ChatPremInput extends BaseChatModelParams {\n    project_id?: number | string;\n    session_id?: string;\n    messages?: {\n        role: \"user\" | \"assistant\";\n        content: string;\n        [k: string]: unknown;\n    }[];\n    model?: string;\n    system_prompt?: string;\n    frequency_penalty?: number;\n    logit_bias?: {\n        [k: string]: unknown;\n    };\n    max_tokens?: number;\n    n?: number;\n    presence_penalty?: number;\n    response_format?: {\n        [k: string]: unknown;\n    };\n    seed?: number;\n    stop?: string;\n    temperature?: number;\n    top_p?: number;\n    tools?: {\n        [k: string]: unknown;\n    }[];\n    user?: string;\n    /**\n     * The Prem API key to use for requests.\n     * @default process.env.PREM_API_KEY\n     */\n    apiKey?: string;\n    streaming?: boolean;\n}\nexport interface ChatCompletionCreateParamsNonStreaming extends CreateChatCompletionRequest {\n    stream?: false;\n}\nexport interface ChatCompletionCreateParamsStreaming extends CreateChatCompletionRequest {\n    stream: true;\n}\nexport type ChatCompletionCreateParams = ChatCompletionCreateParamsNonStreaming | ChatCompletionCreateParamsStreaming;\nexport declare function messageToPremRole(message: BaseMessage): RoleEnum;\n/**\n * Integration with a chat model.\n */\nexport declare class ChatPrem<CallOptions extends BaseLanguageModelCallOptions = BaseLanguageModelCallOptions> extends BaseChatModel<CallOptions> implements ChatPremInput {\n    client: Prem;\n    apiKey?: string;\n    project_id: number;\n    session_id?: string;\n    messages: {\n        [k: string]: unknown;\n        role: \"user\" | \"assistant\";\n        content: string;\n    }[];\n    model?: string;\n    system_prompt?: string;\n    frequency_penalty?: number;\n    logit_bias?: {\n        [k: string]: unknown;\n    };\n    max_tokens?: number;\n    n?: number;\n    presence_penalty?: number;\n    response_format?: {\n        [k: string]: unknown;\n    };\n    seed?: number;\n    stop?: string;\n    temperature?: number;\n    top_p?: number;\n    tools?: {\n        [k: string]: unknown;\n    }[];\n    user?: string;\n    streaming: boolean;\n    [k: string]: unknown;\n    static lc_name(): string;\n    lc_serializable: boolean;\n    /**\n     * Replace with any secrets this class passes to `super`.\n     * See {@link ../../langchain-cohere/src/chat_model.ts} for\n     * an example.\n     */\n    get lc_secrets(): {\n        [key: string]: string;\n    } | undefined;\n    get lc_aliases(): {\n        [key: string]: string;\n    } | undefined;\n    constructor(fields?: ChatPremInput);\n    _llmType(): string;\n    completionWithRetry(request: ChatCompletionCreateParamsStreaming, options?: any): Promise<AsyncIterable<ChatCompletionStreamingCompletionData>>;\n    completionWithRetry(request: ChatCompletionCreateParams, options?: any): Promise<CreateChatCompletionResponse>;\n    invocationParams(options: this[\"ParsedCallOptions\"]): any;\n    /**\n     * Implement to support streaming.\n     * Should yield chunks iteratively.\n     */\n    _streamResponseChunks(messages: BaseMessage[], options: this[\"ParsedCallOptions\"], runManager?: CallbackManagerForLLMRun): AsyncGenerator<ChatGenerationChunk>;\n    /** @ignore */\n    _combineLLMOutput(): never[];\n    _generate(messages: BaseMessage[], options: this[\"ParsedCallOptions\"], runManager?: CallbackManagerForLLMRun): Promise<ChatResult>;\n}\n//# sourceMappingURL=premai.d.ts.map"],"mappings":";;;;;;;;KAMYW,QAAAA;;AAAZ;AAIA;AAmCiBE,UAnCAD,aAAAA,SAAsBT,mBAmCyBI,CAAAA;EAG/CO,UAAAA,CAAAA,EAAAA,MAAAA,GAAAA,MAAAA;EAGLC,UAAAA,CAAAA,EAAAA,MAAAA;EACYC,QAAAA,CAAAA,EAAAA;IAIHC,IAAAA,EAAAA,MAAQ,GAAAC,WAAAA;IAAqBjB,OAAAA,EAAAA,MAAAA;IAA+BA,CAAAA,CAAAA,EAAAA,MAAAA,CAAAA,EAAAA,OAAAA;EAAoDiB,CAAAA,EAAAA;EACzHb,KAAAA,CAAAA,EAAAA,MAAAA;EA4CaO,aAAAA,CAAAA,EAAAA,MAAAA;EAEQE,iBAAAA,CAAAA,EAAAA,MAAAA;EAA2ER,UAAAA,CAAAA,EAAAA;IAAda,CAAAA,CAAAA,EAAAA,MAAAA,CAAAA,EAAAA,OAAAA;EAARC,CAAAA;EACrDL,UAAAA,CAAAA,EAAAA,MAAAA;EAAoDP,CAAAA,CAAAA,EAAAA,MAAAA;EAARY,gBAAAA,CAAAA,EAAAA,MAAAA;EAMzCpB,eAAAA,CAAAA,EAAAA;IAAgEE,CAAAA,CAAAA,EAAAA,MAAAA,CAAAA,EAAAA,OAAAA;EAA0CO,CAAAA;EAAfY,IAAAA,CAAAA,EAAAA,MAAAA;EAGvGrB,IAAAA,CAAAA,EAAAA,MAAAA;EAAgEE,WAAAA,CAAAA,EAAAA,MAAAA;EAAmCQ,KAAAA,CAAAA,EAAAA,MAAAA;EAARU,KAAAA,CAAAA,EAAAA;IAzDIhB,CAAAA,CAAAA,EAAAA,MAAAA,CAAAA,EAAAA,OAAAA;EAAsCQ,CAAAA,EAAAA;EAAa,IAAA,CAAA,EAAA,MAAA;;;;;;;;UAXzJC,sCAAAA,SAA+CN;;;UAG/CO,mCAAAA,SAA4CP;;;KAGjDQ,0BAAAA,GAA6BF,yCAAyCC;iBAC1DE,iBAAAA,UAA2BhB,cAAcW;;;;cAI5CM,6BAA6BhB,+BAA+BA,sCAAsCG,cAAcc,wBAAwBN;UACjJP;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;uBA4CaO;;+BAEQE,qDAAqDM,QAAQD,cAAcb;+BAC3ES,4CAA4CK,QAAQZ;;;;;;kCAMjDR,gEAAgEE,2BAA2BmB,eAAeZ;;;sBAGtHT,gEAAgEE,2BAA2BkB,QAAQV"}