{"version":3,"file":"llama_cpp.d.ts","names":["LlamaModel","LlamaContext","LlamaChatSession","LlamaJsonSchemaGrammar","LlamaGrammar","GbnfJsonSchema","LLM","BaseLLMCallOptions","BaseLLMParams","CallbackManagerForLLMRun","GenerationChunk","LlamaBaseCppInputs","LlamaCppInputs","LlamaCppCallOptions","LlamaCpp","Promise","AsyncGenerator"],"sources":["../../src/llms/llama_cpp.d.ts"],"sourcesContent":["import { LlamaModel, LlamaContext, LlamaChatSession, LlamaJsonSchemaGrammar, LlamaGrammar, GbnfJsonSchema } from \"node-llama-cpp\";\nimport { LLM, type BaseLLMCallOptions, type BaseLLMParams } from \"@langchain/core/language_models/llms\";\nimport { CallbackManagerForLLMRun } from \"@langchain/core/callbacks/manager\";\nimport { GenerationChunk } from \"@langchain/core/outputs\";\nimport { LlamaBaseCppInputs } from \"../utils/llama_cpp.js\";\n/**\n * Note that the modelPath is the only required parameter. For testing you\n * can set this in the environment variable `LLAMA_PATH`.\n */\nexport interface LlamaCppInputs extends LlamaBaseCppInputs, BaseLLMParams {\n}\nexport interface LlamaCppCallOptions extends BaseLLMCallOptions {\n    /** The maximum number of tokens the response should contain. */\n    maxTokens?: number;\n    /** A function called when matching the provided token array */\n    onToken?: (tokens: number[]) => void;\n}\n/**\n *  To use this model you need to have the `node-llama-cpp` module installed.\n *  This can be installed using `npm install -S node-llama-cpp` and the minimum\n *  version supported in version 2.0.0.\n *  This also requires that have a locally built version of Llama3 installed.\n */\nexport declare class LlamaCpp extends LLM<LlamaCppCallOptions> {\n    lc_serializable: boolean;\n    static inputs: LlamaCppInputs;\n    maxTokens?: number;\n    temperature?: number;\n    topK?: number;\n    topP?: number;\n    trimWhitespaceSuffix?: boolean;\n    _model: LlamaModel;\n    _context: LlamaContext;\n    _session: LlamaChatSession;\n    _jsonSchema: LlamaJsonSchemaGrammar<GbnfJsonSchema> | undefined;\n    _gbnf: LlamaGrammar | undefined;\n    static lc_name(): string;\n    constructor(inputs: LlamaCppInputs);\n    /**\n     * Initializes the llama_cpp model for usage.\n     * @param inputs - the inputs passed onto the model.\n     * @returns A Promise that resolves to the LlamaCpp type class.\n     */\n    static initialize(inputs: LlamaCppInputs): Promise<LlamaCpp>;\n    _llmType(): string;\n    /** @ignore */\n    _call(prompt: string, options?: this[\"ParsedCallOptions\"]): Promise<string>;\n    _streamResponseChunks(prompt: string, _options: this[\"ParsedCallOptions\"], runManager?: CallbackManagerForLLMRun): AsyncGenerator<GenerationChunk>;\n}\n//# sourceMappingURL=llama_cpp.d.ts.map"],"mappings":";;;;;;;;;;AASA;AAEA;AAYqBc,UAdJF,cAAAA,SAAuBD,kBAcX,EAd+BH,aAc/B,CAAA;AAEVI,UAdFC,mBAAAA,SAA4BN,kBAc1BK,CAAAA;EAMPZ;EACEC,SAAAA,CAAAA,EAAAA,MAAAA;EACAC;EAC0BG,OAAAA,CAAAA,EAAAA,CAAAA,MAAAA,EAAAA,MAAAA,EAAAA,EAAAA,GAAAA,IAAAA;;;;;;;;AAaoDI,cAxBvEK,QAAAA,SAAiBR,GAwBsDG,CAxBlDI,mBAwBkDJ,CAAAA,CAAAA;EAA0CC,eAAAA,EAAAA,OAAAA;EAAfM,OAAAA,MAAAA,EAtBpGJ,cAsBoGI;EAxBjFV,SAAAA,CAAAA,EAAAA,MAAAA;EAAG,WAAA,CAAA,EAAA,MAAA;;;;UAQ7BN;YACEC;YACAC;eACGC,uBAAuBE;SAC7BD;;sBAEaQ;;;;;;4BAMMA,iBAAiBG,QAAQD;;;8DAGSC;0FAC4BN,2BAA2BO,eAAeN"}