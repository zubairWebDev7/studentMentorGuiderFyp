{"version":3,"file":"llama_cpp.d.ts","names":["LlamaModel","LlamaContext","LlamaEmbeddingContext","LlamaChatSession","LlamaJsonSchemaGrammar","LlamaGrammar","GbnfJsonSchema","Llama","LlamaBaseCppInputs","createLlamaModel","Promise","createLlamaContext","createLlamaEmbeddingContext","createLlamaSession","createLlamaJsonSchemaGrammar","createCustomGrammar"],"sources":["../../src/utils/llama_cpp.d.ts"],"sourcesContent":["import { LlamaModel, LlamaContext, LlamaEmbeddingContext, LlamaChatSession, LlamaJsonSchemaGrammar, LlamaGrammar, GbnfJsonSchema, Llama } from \"node-llama-cpp\";\n/**\n * Note that the modelPath is the only required parameter. For testing you\n * can set this in the environment variable `LLAMA_PATH`.\n */\nexport interface LlamaBaseCppInputs {\n    /** Prompt processing batch size. */\n    batchSize?: number;\n    /** Text context size. */\n    contextSize?: number;\n    /** Embedding mode only. */\n    embedding?: boolean;\n    /** Use fp16 for KV cache. */\n    f16Kv?: boolean;\n    /** Number of layers to store in VRAM. */\n    gpuLayers?: number;\n    /** The llama_eval() call computes all logits, not just the last one. */\n    logitsAll?: boolean;\n    /** */\n    maxTokens?: number;\n    /** Path to the model on the filesystem. */\n    modelPath: string;\n    /** Add the begining of sentence token.  */\n    prependBos?: boolean;\n    /** If null, a random seed will be used. */\n    seed?: null | number;\n    /** The randomness of the responses, e.g. 0.1 deterministic, 1.5 creative, 0.8 balanced, 0 disables. */\n    temperature?: number;\n    /** Number of threads to use to evaluate tokens. */\n    threads?: number;\n    /** Trim whitespace from the end of the generated text Disabled by default. */\n    trimWhitespaceSuffix?: boolean;\n    /** Consider the n most likely tokens, where n is 1 to vocabulary size, 0 disables (uses full vocabulary). Note: only applies when `temperature` > 0. */\n    topK?: number;\n    /** Selects the smallest token set whose probability exceeds P, where P is between 0 - 1, 1 disables. Note: only applies when `temperature` > 0. */\n    topP?: number;\n    /** Force system to keep model in RAM. */\n    useMlock?: boolean;\n    /** Use mmap if possible. */\n    useMmap?: boolean;\n    /** Only load the vocabulary, no weights. */\n    vocabOnly?: boolean;\n    /** JSON schema to be used to format output. Also known as `grammar`. */\n    jsonSchema?: object;\n    /** GBNF string to be used to format output. Also known as `grammar`. */\n    gbnf?: string;\n}\nexport declare function createLlamaModel(inputs: LlamaBaseCppInputs, llama: Llama): Promise<LlamaModel>;\nexport declare function createLlamaContext(model: LlamaModel, inputs: LlamaBaseCppInputs): Promise<LlamaContext>;\nexport declare function createLlamaEmbeddingContext(model: LlamaModel, inputs: LlamaBaseCppInputs): Promise<LlamaEmbeddingContext>;\nexport declare function createLlamaSession(context: LlamaContext): LlamaChatSession;\nexport declare function createLlamaJsonSchemaGrammar(schemaString: object | undefined, llama: Llama): Promise<LlamaJsonSchemaGrammar<GbnfJsonSchema> | undefined>;\nexport declare function createCustomGrammar(filePath: string | undefined, llama: Llama): Promise<LlamaGrammar | undefined>;\n//# sourceMappingURL=llama_cpp.d.ts.map"],"mappings":";;;;;;AAKA;;UAAiBQ,kBAAAA"}