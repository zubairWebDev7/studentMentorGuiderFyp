{"version":3,"file":"base.js","names":["config: CreateOpenAIFnRunnableConfig<RunInput, RunOutput>","llmKwargs: BaseFunctionCallOptions","config: CreateStructuredOutputRunnableConfig<RunInput, RunOutput>","oaiFunction: FunctionDefinition"],"sources":["../../../src/chains/openai_functions/base.ts"],"sourcesContent":["import type { BaseOutputParser } from \"@langchain/core/output_parsers\";\nimport type { BasePromptTemplate } from \"@langchain/core/prompts\";\nimport type { Runnable, RunnableInterface } from \"@langchain/core/runnables\";\nimport type {\n  BaseFunctionCallOptions,\n  BaseLanguageModelInput,\n  FunctionDefinition,\n} from \"@langchain/core/language_models/base\";\nimport {\n  isInteropZodSchema,\n  type InputValues,\n  InteropZodObject,\n} from \"@langchain/core/utils/types\";\nimport type { BaseMessage } from \"@langchain/core/messages\";\nimport {\n  toJsonSchema,\n  type JsonSchema7Type,\n} from \"@langchain/core/utils/json_schema\";\nimport { JsonOutputFunctionsParser } from \"../../output_parsers/openai_functions.js\";\n\n/**\n * Configuration params for the createOpenAIFnRunnable method.\n */\nexport type CreateOpenAIFnRunnableConfig<\n  // eslint-disable-next-line @typescript-eslint/no-explicit-any\n  RunInput extends Record<string, any>,\n  RunOutput,\n> = {\n  functions: FunctionDefinition[];\n  /** Language model to use, assumed to support the OpenAI function-calling API. */\n  llm: RunnableInterface<\n    BaseLanguageModelInput,\n    BaseMessage,\n    BaseFunctionCallOptions\n  >;\n  /** BasePromptTemplate to pass to the model. */\n  prompt: BasePromptTemplate<InputValues<Extract<keyof RunInput, string>>>;\n  /**\n   * Only used if a single function is passed in. If `true`, then the model will be\n   * forced to use the given function. If `false`, then the model will be given the\n   * option to use the given function or not.\n   */\n  enforceSingleFunctionUsage?: boolean;\n  /**\n   * BaseLLMOutputParser to use for parsing model outputs.\n   * By default will be inferred from the function types.\n   */\n  outputParser?: BaseOutputParser<RunOutput>;\n};\n\n/**\n * Creates a runnable sequence that calls OpenAI functions.\n * @param config - The parameters required to create the runnable.\n * @returns A runnable sequence that will pass the given functions to the model when run.\n *\n * @example\n * ```typescript\n * const openAIFunction = {\n *   name: \"get_person_details\",\n *   description: \"Get details about a person\",\n *   parameters: {\n *     title: \"Person\",\n *     description: \"Identifying information about a person.\",\n *     type: \"object\",\n *     properties: {\n *       name: { title: \"Name\", description: \"The person's name\", type: \"string\" },\n *       age: { title: \"Age\", description: \"The person's age\", type: \"integer\" },\n *       fav_food: {\n *         title: \"Fav Food\",\n *         description: \"The person's favorite food\",\n *         type: \"string\",\n *       },\n *     },\n *     required: [\"name\", \"age\"],\n *   },\n * };\n *\n * const model = new ChatOpenAI();\n * const prompt = ChatPromptTemplate.fromMessages([\n *   [\"human\", \"Human description: {description}\"],\n * ]);\n * const outputParser = new JsonOutputFunctionsParser();\n *\n * const runnable = createOpenAIFnRunnable({\n *   functions: [openAIFunction],\n *   llm: model,\n *   prompt,\n *   enforceSingleFunctionUsage: true, // Default is true\n *   outputParser\n * });\n * const response = await runnable.invoke({\n *   description:\n *     \"My name's John Doe and I'm 30 years old. My favorite kind of food are chocolate chip cookies.\",\n * });\n *\n * console.log(response);\n *\n * // { name: 'John Doe', age: 30, fav_food: 'chocolate chip cookies' }\n * ```\n */\nexport function createOpenAIFnRunnable<\n  // eslint-disable-next-line @typescript-eslint/no-explicit-any\n  RunInput extends Record<string, any> = Record<string, any>,\n  // eslint-disable-next-line @typescript-eslint/no-explicit-any\n  RunOutput extends Record<string, any> = Record<string, any>,\n>(\n  config: CreateOpenAIFnRunnableConfig<RunInput, RunOutput>\n): Runnable<RunInput, RunOutput> {\n  const {\n    functions,\n    llm,\n    prompt,\n    enforceSingleFunctionUsage = true,\n    outputParser = new JsonOutputFunctionsParser<RunOutput>(),\n  } = config;\n  const llmKwargs: BaseFunctionCallOptions = {\n    functions,\n  };\n\n  if (functions.length === 1 && enforceSingleFunctionUsage) {\n    llmKwargs.function_call = {\n      name: functions[0].name,\n    };\n  }\n\n  const llmWithKwargs = (llm as Runnable).withConfig(llmKwargs);\n  return prompt.pipe(llmWithKwargs).pipe(outputParser);\n}\n\n/**\n * Configuration params for the createStructuredOutputRunnable method.\n */\nexport type CreateStructuredOutputRunnableConfig<\n  // eslint-disable-next-line @typescript-eslint/no-explicit-any\n  RunInput extends Record<string, any>,\n  RunOutput,\n> = {\n  /**\n   * Schema to output. Must be either valid JSONSchema or a Zod schema.\n   */\n  outputSchema: InteropZodObject | JsonSchema7Type;\n  /**\n   * Language model to use, assumed to support the OpenAI function-calling API.\n   */\n  llm: RunnableInterface<\n    BaseLanguageModelInput,\n    BaseMessage,\n    BaseFunctionCallOptions\n  >;\n  /** BasePromptTemplate to pass to the model. */\n  prompt: BasePromptTemplate<InputValues<Extract<keyof RunInput, string>>>;\n  /**\n   * BaseLLMOutputParser to use for parsing model outputs.\n   */\n  outputParser?: BaseOutputParser<RunOutput>;\n};\n\n/**\n * @deprecated Prefer the `.withStructuredOutput` method on chat model classes.\n *\n * Create a runnable that uses an OpenAI function to get a structured output.\n * @param config Params required to create the runnable.\n * @returns A runnable sequence that will pass the given function to the model when run.\n *\n * @example\n * ```typescript\n * import { createStructuredOutputRunnable } from \"@langchain/classic/chains/openai_functions\";\n * import { ChatOpenAI } from \"@langchain/openai\";\n * import { ChatPromptTemplate } from \"@langchain/core/prompts\";\n * import { JsonOutputFunctionsParser } from \"@langchain/classic/output_parsers\";\n *\n * const jsonSchema = {\n *   title: \"Person\",\n *   description: \"Identifying information about a person.\",\n *   type: \"object\",\n *   properties: {\n *     name: { title: \"Name\", description: \"The person's name\", type: \"string\" },\n *     age: { title: \"Age\", description: \"The person's age\", type: \"integer\" },\n *     fav_food: {\n *       title: \"Fav Food\",\n *       description: \"The person's favorite food\",\n *       type: \"string\",\n *     },\n *   },\n *   required: [\"name\", \"age\"],\n * };\n *\n * const model = new ChatOpenAI({ model: \"gpt-4o-mini\" });\n * const prompt = ChatPromptTemplate.fromMessages([\n *   [\"human\", \"Human description: {description}\"],\n * ]);\n *\n * const outputParser = new JsonOutputFunctionsParser();\n *\n * // Also works with Zod schema\n * const runnable = createStructuredOutputRunnable({\n *   outputSchema: jsonSchema,\n *   llm: model,\n *   prompt,\n *   outputParser\n * });\n *\n * const response = await runnable.invoke({\n *   description:\n *     \"My name's John Doe and I'm 30 years old. My favorite kind of food are chocolate chip cookies.\",\n * });\n *\n * console.log(response);\n *\n * // { name: 'John Doe', age: 30, fav_food: 'chocolate chip cookies' }\n * ```\n */\nexport function createStructuredOutputRunnable<\n  // eslint-disable-next-line @typescript-eslint/no-explicit-any\n  RunInput extends Record<string, any> = Record<string, any>,\n  // eslint-disable-next-line @typescript-eslint/no-explicit-any\n  RunOutput extends Record<string, any> = Record<string, any>,\n>(\n  config: CreateStructuredOutputRunnableConfig<RunInput, RunOutput>\n): Runnable<RunInput, RunOutput> {\n  const { outputSchema, llm, prompt, outputParser } = config;\n  const jsonSchema = isInteropZodSchema(outputSchema)\n    ? toJsonSchema(outputSchema)\n    : outputSchema;\n  const oaiFunction: FunctionDefinition = {\n    name: \"outputFormatter\",\n    description:\n      \"Output formatter. Should always be used to format your response to the user\",\n    parameters: jsonSchema,\n  };\n\n  return createOpenAIFnRunnable({\n    functions: [oaiFunction],\n    llm,\n    prompt,\n    enforceSingleFunctionUsage: true,\n    outputParser,\n  });\n}\n"],"mappings":";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;AAoGA,SAAgB,uBAMdA,QAC+B;CAC/B,MAAM,EACJ,WACA,KACA,QACA,6BAA6B,MAC7B,eAAe,IAAI,6BACpB,GAAG;CACJ,MAAMC,YAAqC,EACzC,UACD;AAED,KAAI,UAAU,WAAW,KAAK,4BAC5B,UAAU,gBAAgB,EACxB,MAAM,UAAU,GAAG,KACpB;CAGH,MAAM,gBAAiB,IAAiB,WAAW,UAAU;AAC7D,QAAO,OAAO,KAAK,cAAc,CAAC,KAAK,aAAa;AACrD;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;AAqFD,SAAgB,+BAMdC,QAC+B;CAC/B,MAAM,EAAE,cAAc,KAAK,QAAQ,cAAc,GAAG;CACpD,MAAM,aAAa,mBAAmB,aAAa,GAC/C,aAAa,aAAa,GAC1B;CACJ,MAAMC,cAAkC;EACtC,MAAM;EACN,aACE;EACF,YAAY;CACb;AAED,QAAO,uBAAuB;EAC5B,WAAW,CAAC,WAAY;EACxB;EACA;EACA,4BAA4B;EAC5B;CACD,EAAC;AACH"}